@misc{parsingunveiled,
  title         = {Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction},
  author        = {Qintong Zhang and Bin Wang and Victor Shea-Jay Huang and Junyuan Zhang and Zhengren Wang and Hao Liang and Conghui He and Wentao Zhang},
  year          = {2025},
  eprint        = {2410.21169},
  archiveprefix = {arXiv},
  primaryclass  = {cs.MM},
  url           = {https://arxiv.org/abs/2410.21169}
}

@misc{docxchain,
  title         = {DocXChain: A Powerful Open-Source Toolchain for Document Parsing and Beyond},
  author        = {Cong Yao},
  year          = {2023},
  eprint        = {2310.12430},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2310.12430}
}

@misc{unstructuredio,
  author  = {{Unstructured.io Team}},
  title   = {Unstructured.io: Open-Source Pre-Processing Tools for Unstructured Data},
  url     = {https://unstructured.io},
  urldate = {2026-01-20}
}

@misc{yolox,
  title         = {YOLOX: Exceeding YOLO Series in 2021},
  author        = {Zheng Ge and Songtao Liu and Feng Wang and Zeming Li and Jian Sun},
  year          = {2021},
  eprint        = {2107.08430},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2107.08430}
}

@misc{unstructured_open_source,
  author  = {{Unstructured.io Team}},
  title   = {Unstructured.io: Documentation for the open-source library},
  url     = {https://docs.unstructured.io/open-source/introduction/overview},
  urldate = {2026-01-28}
}

@techreport{docling,
  author  = {Deep Search Team},
  month   = {8},
  title   = {Docling Technical Report},
  url     = {https://arxiv.org/abs/2408.09869},
  eprint  = {2408.09869},
  doi     = {10.48550/arXiv.2408.09869},
  version = {1.0.0},
  year    = {2024}
}

@misc{docling_heron,
  title         = {Advanced Layout Analysis Models for Docling},
  author        = {Nikolaos Livathinos and Christoph Auer and Ahmed Nassar and Rafael Teixeira de Lima and Maksym Lysak and Brown Ebouky and Cesar Berrospi and Michele Dolfi and Panagiotis Vagenas and Matteo Omenetti and Kasper Dinkla and Yusik Kim and Valery Weber and Lucas Morin and Ingmar Meijer and Viktor Kuropiatnyk and Tim Strohmeyer and A. Said Gurbuz and Peter W. J. Staar},
  year          = {2025},
  eprint        = {2509.11720},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2509.11720}
}

@misc{mineru_vlm,
  title         = {MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing},
  author        = {Junbo Niu and Zheng Liu and Zhuangcheng Gu and Bin Wang and Linke Ouyang and Zhiyuan Zhao and Tao Chu and Tianyao He and Fan Wu and Qintong Zhang and Zhenjiang Jin and Guang Liang and Rui Zhang and Wenzheng Zhang and Yuan Qu and Zhifei Ren and Yuefeng Sun and Yuanhong Zheng and Dongsheng Ma and Zirui Tang and Boyu Niu and Ziyang Miao and Hejun Dong and Siyi Qian and Junyuan Zhang and Jingzhou Chen and Fangdong Wang and Xiaomeng Zhao and Liqun Wei and Wei Li and Shasha Wang and Ruiliang Xu and Yuanyuan Cao and Lu Chen and Qianqian Wu and Huaiyu Gu and Lindong Lu and Keming Wang and Dechen Lin and Guanlin Shen and Xuanhe Zhou and Linfeng Zhang and Yuhang Zang and Xiaoyi Dong and Jiaqi Wang and Bo Zhang and Lei Bai and Pei Chu and Weijia Li and Jiang Wu and Lijun Wu and Zhenxiang Li and Guangyu Wang and Zhongying Tu and Chao Xu and Kai Chen and Yu Qiao and Bowen Zhou and Dahua Lin and Wentao Zhang and Conghui He},
  year          = {2025},
  eprint        = {2509.22186},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2509.22186}
}

@misc{mineru,
  title         = {MinerU: An Open-Source Solution for Precise Document Content Extraction},
  author        = {Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
  year          = {2024},
  eprint        = {2409.18839},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2409.18839}
}

@article{opendatalab,
  title   = {Opendatalab: Empowering general artificial intelligence with open datasets},
  author  = {He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},
  journal = {arXiv preprint arXiv:2407.13773},
  year    = {2024}
}

@misc{docbank,
  title         = {DocBank: A Benchmark Dataset for Document Layout Analysis},
  author        = {Minghao Li and Yiheng Xu and Lei Cui and Shaohan Huang and Furu Wei and Zhoujun Li and Ming Zhou},
  year          = {2020},
  eprint        = {2006.01038},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2006.01038}
}

@article{doclaynet,
  title  = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis},
  doi    = {10.1145/3534678.353904},
  url    = {https://arxiv.org/abs/2206.01062},
  author = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter W J},
  year   = {2022}
}

@inproceedings{publaynet,
  title        = {PubLayNet: largest dataset ever for document layout analysis},
  author       = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
  booktitle    = {2019 International Conference on Document Analysis and Recognition (ICDAR)},
  year         = {2019},
  volume       = {},
  number       = {},
  pages        = {1015-1022},
  doi          = {10.1109/ICDAR.2019.00166},
  issn         = {1520-5363},
  month        = {9},
  organization = {IEEE}
}

@misc{publaynet-mini,
  author       = {Kenza Benkirane},
  title        = {publaynet-mini},
  year         = {2029},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/datasets/kenza-ily/publaynet-mini}}
}

@misc{omnidocbench,
  title         = {OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations},
  author        = {Linke Ouyang and Yuan Qu and Hongbin Zhou and Jiawei Zhu and Rui Zhang and Qunshu Lin and Bin Wang and Zhiyuan Zhao and Man Jiang and Xiaomeng Zhao and Jin Shi and Fan Wu and Pei Chu and Minghao Liu and Zhenxiang Li and Chao Xu and Bo Zhang and Botian Shi and Zhongying Tu and Conghui He},
  year          = {2024},
  eprint        = {2412.07626},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2412.07626}
}

@inproceedings{icdar2009,
  author    = {Antonacopoulos, Apostolos and Pletschacher, Stefan and Bridson, David and Papadopoulos, Christos},
  booktitle = {2009 10th International Conference on Document Analysis and Recognition},
  title     = {ICDAR 2009 Page Segmentation Competition},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {1370-1374},
  keywords  = {Image segmentation;Image analysis;Text analysis;Pattern analysis;Pattern recognition;Image recognition;Art;Robustness;Image enhancement;Pixel},
  doi       = {10.1109/ICDAR.2009.275}
}

@misc{icdar2021_competition,
  title         = {ICDAR 2021 Competition on Scientific Literature Parsing},
  author        = {Antonio Jimeno Yepes and Xu Zhong and Douglas Burdick},
  year          = {2021},
  eprint        = {2106.14616},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2106.14616}
}

@article{object_detection,
  title    = {A comprehensive review of object detection with deep learning},
  journal  = {Digital Signal Processing},
  volume   = {132},
  pages    = {103812},
  year     = {2023},
  issn     = {1051-2004},
  doi      = {https://doi.org/10.1016/j.dsp.2022.103812},
  url      = {https://www.sciencedirect.com/science/article/pii/S1051200422004298},
  author   = {Ravpreet Kaur and Sarbjeet Singh},
  keywords = {Computer vision, Deep convolutional neural network, Object detection, Deep learning, Conventional methods},
  abstract = {In the realm of computer vision, Deep Convolutional Neural Networks (DCNNs) have demonstrated excellent performance. Video Processing, Object Detection, Image Segmentation, Image Classification, Speech Recognition and Natural Language Processing are some of the application areas of CNN. Object detection is the most crucial and challenging task of computer vision. It has numerous applications in the field of security, military, transportation and medical sciences. In this review, object detection and its different aspects have been covered in detail. With the gradual increase in the evolution of deep learning algorithms for detecting objects, a significant improvement in the performance of object detection models has been observed. However, this does not imply that the conventional object detection methods, which had been evolving for decades prior to the emergence of deep learning, had become outdated. There are some cases where conventional methods with global features are superior choice. This review paper starts with a quick overview of object detection followed by object detection frameworks, backbone convolutional neural network, and an overview of common datasets along with the evaluation metrics. Object detection problems and applications are also studied in detail. Some future research challenges in designing deep neural networks are discussed. Lastly, the performance of object detection models on PASCAL VOC and MS COCO datasets is compared and conclusions are drawn.}
}

@article{object_detection_metrics,
  author         = {Padilla, Rafael and Passos, Wesley L. and Dias, Thadeu L. B. and Netto, Sergio L. and da Silva, Eduardo A. B.},
  title          = {A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit},
  journal        = {Electronics},
  volume         = {10},
  year           = {2021},
  number         = {3},
  article-number = {279},
  url            = {https://www.mdpi.com/2079-9292/10/3/279},
  issn           = {2079-9292},
  abstract       = {Recent outstanding results of supervised object detection in competitions and challenges are often associated with specific metrics and datasets. The evaluation of such methods applied in different contexts have increased the demand for annotated datasets. Annotation tools represent the location and size of objects in distinct formats, leading to a lack of consensus on the representation. Such a scenario often complicates the comparison of object detection methods. This work alleviates this problem along the following lines: (i) It provides an overview of the most relevant evaluation methods used in object detection competitions, highlighting their peculiarities, differences, and advantages; (ii) it examines the most used annotation formats, showing how different implementations may influence the assessment results; and (iii) it provides a novel open-source toolkit supporting different annotation formats and 15 performance metrics, making it easy for researchers to evaluate the performance of their detection algorithms in most known datasets. In addition, this work proposes a new metric, also included in the toolkit, for evaluating object detection in videos that is based on the spatio-temporal overlap between the ground-truth and detected bounding boxes.},
  doi            = {10.3390/electronics10030279}
}


@misc{object_detection_survey,
  title         = {Object Detection in 20 Years: A Survey},
  author        = {Zhengxia Zou and Keyan Chen and Zhenwei Shi and Yuhong Guo and Jieping Ye},
  year          = {2023},
  eprint        = {1905.05055},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1905.05055}
}

@article{precision_recall,
  title   = {Recall, precision and average precision},
  author  = {Zhu, Mu},
  journal = {Department of Statistics and Actuarial Science, University of Waterloo, Waterloo},
  volume  = {2},
  number  = {30},
  pages   = {6},
  year    = {2004}
}

@misc{lrp_error,
  title         = {One Metric to Measure them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks},
  author        = {Kemal Oksuz and Baris Can Cam and Sinan Kalkan and Emre Akbas},
  year          = {2021},
  eprint        = {2011.10772},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2011.10772}
}

@article{eclair,
  title   = {$\backslash$'Eclair--Extracting Content and Layout with Integrated Reading Order for Documents},
  author  = {Karmanov, Ilia and Deshmukh, Amala Sanjay and Voegtle, Lukas and Fischer, Philipp and Chumachenko, Kateryna and Roman, Timo and Sepp{\"a}nen, Jarno and Parmar, Jupinder and Jennings, Joseph and Tao, Andrew and others},
  journal = {arXiv preprint arXiv:2502.04223},
  year    = {2025}
}

@inproceedings{scenescript,
  title        = {Scenescript: Reconstructing scenes with an autoregressive structured language model},
  author       = {Avetisyan, Armen and Xie, Christopher and Howard-Jenkins, Henry and Yang, Tsun-Yi and Aroudj, Samir and Patra, Suvam and Zhang, Fuyang and Frost, Duncan and Holland, Luke and Orme, Campbell and others},
  booktitle    = {European Conference on Computer Vision},
  pages        = {247--263},
  year         = {2024},
  organization = {Springer}
}

@misc{maximize_f1,
  title         = {Thresholding Classifiers to Maximize F1 Score},
  author        = {Zachary Chase Lipton and Charles Elkan and Balakrishnan Narayanaswamy},
  year          = {2014},
  eprint        = {1402.1892},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1402.1892}
}

@inproceedings{coco,
  title        = {Microsoft coco: Common objects in context},
  author       = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle    = {European conference on computer vision},
  pages        = {740--755},
  year         = {2014},
  organization = {Springer}
}

@article{faster-coco-eval,
  title  = {{Faster-COCO-Eval}: Faster and Enhanced COCO Evaluation Library},
  author = {MiXaiLL76},
  year   = {2024}
}

@software{llamaindex,
  author = {Liu, Jerry},
  doi    = {10.5281/zenodo.1234},
  month  = {11},
  title  = {{LlamaIndex}},
  url    = {https://github.com/jerryjliu/llama_index},
  year   = {2022}
}

@software{langchain,
  author       = {Harrison Chase},
  title        = {LangChain},
  month        = {10},
  year         = {2022},
  howpublished = {\url{https://github.com/langchain-ai/langchain}},
  url          = {https://github.com/langchain-ai/langchain},
  commit       = {October 17, 2022}
}

@online{langchain_splitting_recursively,
  author  = {{LangChain Inc.}},
  title   = {LangChain Docs: Splitting recursively},
  url     = {https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter},
  urldate = {2026-01-27}
}

@misc{docling_toolkit,
  title         = {Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion},
  author        = {Nikolaos Livathinos and Christoph Auer and Maksym Lysak and Ahmed Nassar and Michele Dolfi and Panos Vagenas and Cesar Berrospi Ramis and Matteo Omenetti and Kasper Dinkla and Yusik Kim and Shubham Gupta and Rafael Teixeira de Lima and Valery Weber and Lucas Morin and Ingmar Meijer and Viktor Kuropiatnyk and Peter W. J. Staar},
  year          = {2025},
  eprint        = {2501.17887},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2501.17887}
}

@inproceedings{fast_tokenization,
  title     = {Fast wordpiece tokenization},
  author    = {Song, Xinying and Salcianu, Alex and Song, Yang and Dopson, Dave and Zhou, Denny},
  booktitle = {Proceedings of the 2021 conference on empirical methods in natural language processing},
  pages     = {2089--2103},
  year      = {2021}
}

@misc{token_history,
  title         = {Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP},
  author        = {Sabrina J. Mielke and Zaid Alyafeai and Elizabeth Salesky and Colin Raffel and Manan Dey and Matthias Gallé and Arun Raja and Chenglei Si and Wilson Y. Lee and Benoît Sagot and Samson Tan},
  year          = {2021},
  eprint        = {2112.10508},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2112.10508}
}

@inproceedings{wordpiece,
  author    = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle = {2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Japanese and Korean voice search},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {5149-5152},
  keywords  = {Decision support systems;Helium;Speech recognition;voice search;Japanese;Korean},
  doi       = {10.1109/ICASSP.2012.6289079}
}

@inproceedings{unigram,
  title     = {Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
  author    = {Kudo, Taku},
  editor    = {Gurevych, Iryna  and
               Miyao, Yusuke},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P18-1007/},
  doi       = {10.18653/v1/P18-1007},
  pages     = {66--75},
  abstract  = {Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.}
}

@inproceedings{sentencepiece,
  title     = {{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author    = {Kudo, Taku  and
               Richardson, John},
  editor    = {Blanco, Eduardo  and
               Lu, Wei},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D18-2012/},
  doi       = {10.18653/v1/D18-2012},
  pages     = {66--71},
  abstract  = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.}
}

@article{guidelines_length,
  title     = {Changes in length and complexity of clinical practice guidelines in oncology, 1996-2019},
  author    = {Kann, Benjamin H and Johnson, Skyler B and Aerts, Hugo JWL and Mak, Raymond H and Nguyen, Paul L},
  journal   = {JAMA Network Open},
  volume    = {3},
  number    = {3},
  pages     = {e200841--e200841},
  year      = {2020},
  publisher = {American Medical Association}
}

@misc{rag,
  title         = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author        = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
  year          = {2021},
  eprint        = {2005.11401},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2005.11401}
}

@misc{rag_survey,
  title         = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  author        = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
  year          = {2024},
  eprint        = {2312.10997},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2312.10997}
}

@inproceedings{chunk_size_effect_on_rag,
  author    = {Hlad{\v{e}}na, Jan
               and {\v{S}}teflovi{\v{c}}, Kirsten
               and {\v{C}}ech, Pavel
               and {\v{S}}tekerov{\'a}, Kamila
               and {\v{Z}}v{\'a}{\v{c}}kov{\'a}, Andrea},
  editor    = {Silhavy, Radek
               and Silhavy, Petr},
  title     = {The Effect of Chunk Size on the RAG Performance},
  booktitle = {Software Engineering: Emerging Trends and Practices in System Development},
  year      = {2025},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {317--326},
  abstract  = {Retrieval-Augmented Generation (RAG) is an emerging paradigm that enhances the performance of Large Language Models (LLMs) by integrating external knowledge retrieval into their generative processes. One of the key challenges in optimizing RAG systems is the selection of an appropriate retrieval dataset, which directly affects model accuracy, retrieval efficiency, and response coherence. The chunk size plays a fundamental role in the performance of RAG systems, as it directly impacts retrieval efficiency, response quality, and computational costs. The Neural Bridge RAG Dataset 12000 was used to test the effect of a chunk size in our experiment.},
  isbn      = {978-3-032-00712-4}
}

@article{lins,
  author    = {Wang, Shuo and Zhao, Feng and Bu, Dongbo and et al.},
  title     = {{LINS}: A general medical {Q\&A} framework for enhancing the quality and credibility of {LLM}-generated responses},
  journal   = {Nature Communications},
  year      = {2025},
  volume    = {16},
  number    = {1},
  pages     = {9076},
  month     = {10},
  doi       = {10.1038/s41467-025-64142-2},
  url       = {https://doi.org/10.1038/s41467-025-64142-2},
  publisher = {Nature Publishing Group}
}

@inproceedings{semantic_chunking,
  title     = {Is semantic chunking worth the computational cost?},
  author    = {Qu, Renyi and Tu, Ruixuan and Bao, Forrest},
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2025},
  pages     = {2155--2177},
  year      = {2025}
}

@inproceedings{chunking_comparison,
  author    = {Jaiswal, Shruti and Bisht, Priyank and Kansara, Krity and Datta, MSH Shankar},
  booktitle = {2025 International Conference on Responsible, Generative and Explainable AI (ResGenXAI)},
  title     = {Comparison of Chunking Techniques Across Diverse Document Types in NLP Retrieval Tasks},
  year      = {2025},
  volume    = {},
  number    = {},
  pages     = {1-6},
  keywords  = {Measurement;Accuracy;Systematics;Semantics;Retrieval augmented generation;Pipelines;Buildings;Robustness;Real-time systems;Question answering (information retrieval);Document Chunking;Information Retrieval;Natural Language Processing;Semantic Chunking;Retrieval-Augmented Generation;Document Segmentation},
  doi       = {10.1109/ResgenXAI64788.2025.11344045}
}

@techreport{chroma_eval,
  title       = {Evaluating Chunking Strategies for Retrieval},
  author      = {Smith, Brandon and Troynikov, Anton},
  year        = {2024},
  month       = {6},
  institution = {Chroma},
  url         = {https://research.trychroma.com/evaluating-chunking}
}

@misc{aidvice,
  author  = {{Chair of Software Engineering for Business Information Systems, TUM School of Computation, Information and Technology, Technichal University of Munich}},
  title   = {AI-Based Knowledge Assistant for Cancer Care (Aidvice)},
  year    = {2025},
  url     = {https://www.cs.cit.tum.de/sebis/research/natural-language-processing/ai-based-knowledge-assistant-for-cancer-care-aidvice/},
  urldate = {2026-01-28}
}

@misc{monkeyocr,
  title         = {MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm},
  author        = {Zhang Li and Yuliang Liu and Qiang Liu and Zhiyin Ma and Ziyang Zhang and Shuo Zhang and Zidun Guo and Jiarui Zhang and Xinyu Wang and Xiang Bai},
  year          = {2025},
  eprint        = {2506.05218},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2506.05218}
}

@inproceedings{intelligent_doc_parsing,
  title     = {Intelligent Document Parsing: Towards End-to-end Document Parsing via Decoupled Content Parsing and Layout Grounding},
  author    = {Xing, Hangdi  and
               Gao, Feiyu  and
               Zheng, Qi  and
               Zhu, Zhaoqing  and
               Shao, Zirui  and
               Yan, Ming},
  editor    = {Christodoulopoulos, Christos  and
               Chakraborty, Tanmoy  and
               Rose, Carolyn  and
               Peng, Violet},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2025},
  month     = nov,
  year      = {2025},
  address   = {Suzhou, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.findings-emnlp.1088/},
  doi       = {10.18653/v1/2025.findings-emnlp.1088},
  pages     = {19987--19998},
  isbn      = {979-8-89176-335-7},
  abstract  = {In the daily work, vast amounts of documents are stored in pixel-based formats such as images and scanned PDFs, posing challenges for efficient database management and data processing. Existing methods often fragment the parsing process into the pipeline of separated subtasks on the layout element level, resulting in incomplete semantics and error propagation. Even though models based on multi-modal large language models (MLLMs) mitigate the issues to some extent, they also suffer from absent or sub-optimal grounding ability for visual information. To address these challenges, we introduce the Intelligent Document Parsing (IDP) framework, an end-to-end document parsing framework leveraging the vision-language priors of MLLMs, equipped with an elaborately designed document representation and decoding mechanism to decouple the content parsing and layout grounding to fully activate the potential of MLLMs for document parsing. Experimental results demonstrate that the IDP method surpasses existing methods, significantly advancing MLLM-based document parsing.}
}

@misc{pp_doclayout,
  title         = {PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction},
  author        = {Ting Sun and Cheng Cui and Yuning Du and Yi Liu},
  year          = {2025},
  eprint        = {2503.17213},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2503.17213}
}

@misc{layoutlm_v3,
  title         = {LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
  author        = {Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},
  year          = {2022},
  eprint        = {2204.08387},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2204.08387}
}

@article{yolo,
  author     = {Joseph Redmon and
                Santosh Kumar Divvala and
                Ross B. Girshick and
                Ali Farhadi},
  title      = {You Only Look Once: Unified, Real-Time Object Detection},
  journal    = {CoRR},
  volume     = {abs/1506.02640},
  year       = {2015},
  url        = {http://arxiv.org/abs/1506.02640},
  eprinttype = {arXiv},
  eprint     = {1506.02640},
  timestamp  = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/RedmonDGF15.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{detr,
  author     = {Nicolas Carion and
                Francisco Massa and
                Gabriel Synnaeve and
                Nicolas Usunier and
                Alexander Kirillov and
                Sergey Zagoruyko},
  title      = {End-to-End Object Detection with Transformers},
  journal    = {CoRR},
  volume     = {abs/2005.12872},
  year       = {2020},
  url        = {https://arxiv.org/abs/2005.12872},
  eprinttype = {arXiv},
  eprint     = {2005.12872},
  timestamp  = {Thu, 28 May 2020 17:38:09 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{ocr_survey,
  title   = {A survey on optical character recognition system},
  author  = {Islam, Noman and Islam, Zeeshan and Noor, Nazia},
  journal = {arXiv preprint arXiv:1710.05703},
  year    = {2017}
}

@misc{easyocr,
  author       = {Kittinaradorn, Rakpong and JaidedAI},
  title        = {EasyOCR},
  year         = {2020},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/JaidedAI/EasyOCR}}
}

@inproceedings{tesseract,
  title        = {An overview of the Tesseract OCR engine},
  author       = {Smith, Ray},
  booktitle    = {Ninth international conference on document analysis and recognition (ICDAR 2007)},
  volume       = {2},
  pages        = {629--633},
  year         = {2007},
  organization = {IEEE}
}

@article{impact_dataset_rag_multi_hop,
  title  = {Understanding the Impact of Dataset Characteristics on RAG based Multi-hop QA Performance},
  author = {Aksoy, Nimet and G{\"u}ven, Zekeriya An{\i}l and {\"U}nal{\i}r, Murat Osman},
  year   = {2025}
}

@misc{dense_x,
  title         = {Dense X Retrieval: What Retrieval Granularity Should We Use?},
  author        = {Tong Chen and Hongwei Wang and Sihao Chen and Wenhao Yu and Kaixin Ma and Xinran Zhao and Hongming Zhang and Dong Yu},
  year          = {2024},
  eprint        = {2312.06648},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2312.06648}
}

@misc{data_quality_rag,
  title         = {Data Quality Challenges in Retrieval-Augmented Generation},
  author        = {Leopold Müller and Joshua Holstein and Sarah Bause and Gerhard Satzger and Niklas Kühl},
  year          = {2025},
  eprint        = {2510.00552},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2510.00552}
}

@misc{visa,
  title         = {VISA: Retrieval Augmented Generation with Visual Source Attribution},
  author        = {Xueguang Ma and Shengyao Zhuang and Bevan Koopman and Guido Zuccon and Wenhu Chen and Jimmy Lin},
  year          = {2024},
  eprint        = {2412.14457},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2412.14457}
}

@online{nccn_about_guidelines,
  author       = {{National Comprehensive Cancer Network}},
  title        = {About Clinical Practice Guidelines},
  url          = {https://www.nccn.org/guidelines/guidelines-process/about-nccn-clinical-practice-guidelines},
  urldate      = {2026-01-29},
  organization = {NCCN}
}

@book{cpg_trust,
  title     = {Clinical practice guidelines we can trust},
  author    = {Steinberg, Earl and Greenfield, Sheldon and Wolman, Dianne Miller and Mancher, Michelle and Graham, Robin},
  year      = {2011},
  publisher = {national academies press}
}

@inproceedings{dual_encoders,
  title     = {Large dual encoders are generalizable retrievers},
  author    = {Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and Abrego, Gustavo Hernandez and Ma, Ji and Zhao, Vincent and Luan, Yi and Hall, Keith and Chang, Ming-Wei and others},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages     = {9844--9855},
  year      = {2022}
}

@inproceedings{replug,
  title     = {Replug: Retrieval-augmented black-box language models},
  author    = {Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Richard and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages     = {8371--8384},
  year      = {2024}
}

@article{qwen25vl,
  title   = {Qwen2. 5-vl technical report},
  author  = {Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal = {arXiv preprint arXiv:2502.13923},
  year    = {2025}
}

@misc{vlm_frontier,
  title         = {Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions},
  author        = {Akash Ghosh and Arkadeep Acharya and Sriparna Saha and Vinija Jain and Aman Chadha},
  year          = {2025},
  eprint        = {2404.07214},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2404.07214}
}

@inproceedings{clip,
  title        = {Learning transferable visual models from natural language supervision},
  author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle    = {International conference on machine learning},
  pages        = {8748--8763},
  year         = {2021},
  organization = {PmLR}
}

@article{vlmo,
  title   = {Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
  author  = {Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
  journal = {Advances in neural information processing systems},
  volume  = {35},
  pages   = {32897--32912},
  year    = {2022}
}

@misc{general_ocr_theory,
  title         = {General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model},
  author        = {Haoran Wei and Chenglong Liu and Jinyue Chen and Jia Wang and Lingyu Kong and Yanming Xu and Zheng Ge and Liang Zhao and Jianjian Sun and Yuang Peng and Chunrui Han and Xiangyu Zhang},
  year          = {2024},
  eprint        = {2409.01704},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2409.01704}
}

@misc{dotsocr,
  title         = {dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model},
  author        = {Yumeng Li and Guang Yang and Hao Liu and Bowen Wang and Colin Zhang},
  year          = {2025},
  eprint        = {2512.02498},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2512.02498}
}