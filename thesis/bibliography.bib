@misc{parsingunveiled,
  title         = {Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction},
  author        = {Qintong Zhang and Bin Wang and Victor Shea-Jay Huang and Junyuan Zhang and Zhengren Wang and Hao Liang and Conghui He and Wentao Zhang},
  year          = {2025},
  eprint        = {2410.21169},
  archiveprefix = {arXiv},
  primaryclass  = {cs.MM},
  url           = {https://arxiv.org/abs/2410.21169}
}

@misc{docxchain,
  title         = {DocXChain: A Powerful Open-Source Toolchain for Document Parsing and Beyond},
  author        = {Cong Yao},
  year          = {2023},
  eprint        = {2310.12430},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2310.12430}
}

@misc{unstructuredio,
  author = {Unstructured.io Team},
  title  = {Unstructured.io: Open-Source Pre-Processing Tools for Unstructured Data},
  year   = {2024},
  url    = {https://unstructured.io},
  note   = {Accessed: 2026-01-20}
}

@techreport{docling,
  author  = {Deep Search Team},
  month   = {8},
  title   = {Docling Technical Report},
  url     = {https://arxiv.org/abs/2408.09869},
  eprint  = {2408.09869},
  doi     = {10.48550/arXiv.2408.09869},
  version = {1.0.0},
  year    = {2024}
}

@misc{docling_heron,
  title         = {Advanced Layout Analysis Models for Docling},
  author        = {Nikolaos Livathinos and Christoph Auer and Ahmed Nassar and Rafael Teixeira de Lima and Maksym Lysak and Brown Ebouky and Cesar Berrospi and Michele Dolfi and Panagiotis Vagenas and Matteo Omenetti and Kasper Dinkla and Yusik Kim and Valery Weber and Lucas Morin and Ingmar Meijer and Viktor Kuropiatnyk and Tim Strohmeyer and A. Said Gurbuz and Peter W. J. Staar},
  year          = {2025},
  eprint        = {2509.11720},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2509.11720}
}

@misc{mineru_vlm,
  title         = {MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing},
  author        = {Junbo Niu and Zheng Liu and Zhuangcheng Gu and Bin Wang and Linke Ouyang and Zhiyuan Zhao and Tao Chu and Tianyao He and Fan Wu and Qintong Zhang and Zhenjiang Jin and Guang Liang and Rui Zhang and Wenzheng Zhang and Yuan Qu and Zhifei Ren and Yuefeng Sun and Yuanhong Zheng and Dongsheng Ma and Zirui Tang and Boyu Niu and Ziyang Miao and Hejun Dong and Siyi Qian and Junyuan Zhang and Jingzhou Chen and Fangdong Wang and Xiaomeng Zhao and Liqun Wei and Wei Li and Shasha Wang and Ruiliang Xu and Yuanyuan Cao and Lu Chen and Qianqian Wu and Huaiyu Gu and Lindong Lu and Keming Wang and Dechen Lin and Guanlin Shen and Xuanhe Zhou and Linfeng Zhang and Yuhang Zang and Xiaoyi Dong and Jiaqi Wang and Bo Zhang and Lei Bai and Pei Chu and Weijia Li and Jiang Wu and Lijun Wu and Zhenxiang Li and Guangyu Wang and Zhongying Tu and Chao Xu and Kai Chen and Yu Qiao and Bowen Zhou and Dahua Lin and Wentao Zhang and Conghui He},
  year          = {2025},
  eprint        = {2509.22186},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2509.22186}
}

@misc{mineru,
  title         = {MinerU: An Open-Source Solution for Precise Document Content Extraction},
  author        = {Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
  year          = {2024},
  eprint        = {2409.18839},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2409.18839}
}

@article{opendatalab,
  title   = {Opendatalab: Empowering general artificial intelligence with open datasets},
  author  = {He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},
  journal = {arXiv preprint arXiv:2407.13773},
  year    = {2024}
}

@misc{docbank,
  title         = {DocBank: A Benchmark Dataset for Document Layout Analysis},
  author        = {Minghao Li and Yiheng Xu and Lei Cui and Shaohan Huang and Furu Wei and Zhoujun Li and Ming Zhou},
  year          = {2020},
  eprint        = {2006.01038},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2006.01038}
}

@article{doclaynet,
  title  = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis},
  doi    = {10.1145/3534678.353904},
  url    = {https://arxiv.org/abs/2206.01062},
  author = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter W J},
  year   = {2022}
}

@inproceedings{publaynet,
  title        = {PubLayNet: largest dataset ever for document layout analysis},
  author       = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
  booktitle    = {2019 International Conference on Document Analysis and Recognition (ICDAR)},
  year         = {2019},
  volume       = {},
  number       = {},
  pages        = {1015-1022},
  doi          = {10.1109/ICDAR.2019.00166},
  issn         = {1520-5363},
  month        = {9},
  organization = {IEEE}
}

@misc{publaynet-mini,
  author       = {Kenza Benkirane},
  title        = {publaynet-mini},
  year         = {2029},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/datasets/kenza-ily/publaynet-mini}}
}

@misc{omnidocbench,
  title         = {OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations},
  author        = {Linke Ouyang and Yuan Qu and Hongbin Zhou and Jiawei Zhu and Rui Zhang and Qunshu Lin and Bin Wang and Zhiyuan Zhao and Man Jiang and Xiaomeng Zhao and Jin Shi and Fan Wu and Pei Chu and Minghao Liu and Zhenxiang Li and Chao Xu and Bo Zhang and Botian Shi and Zhongying Tu and Conghui He},
  year          = {2024},
  eprint        = {2412.07626},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2412.07626}
}

@inproceedings{icdar2009,
  author    = {Antonacopoulos, Apostolos and Pletschacher, Stefan and Bridson, David and Papadopoulos, Christos},
  booktitle = {2009 10th International Conference on Document Analysis and Recognition},
  title     = {ICDAR 2009 Page Segmentation Competition},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {1370-1374},
  keywords  = {Image segmentation;Image analysis;Text analysis;Pattern analysis;Pattern recognition;Image recognition;Art;Robustness;Image enhancement;Pixel},
  doi       = {10.1109/ICDAR.2009.275}
}

@misc{icdar2021_competition,
  title         = {ICDAR 2021 Competition on Scientific Literature Parsing},
  author        = {Antonio Jimeno Yepes and Xu Zhong and Douglas Burdick},
  year          = {2021},
  eprint        = {2106.14616},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2106.14616}
}

@article{object_detection,
  title    = {A comprehensive review of object detection with deep learning},
  journal  = {Digital Signal Processing},
  volume   = {132},
  pages    = {103812},
  year     = {2023},
  issn     = {1051-2004},
  doi      = {https://doi.org/10.1016/j.dsp.2022.103812},
  url      = {https://www.sciencedirect.com/science/article/pii/S1051200422004298},
  author   = {Ravpreet Kaur and Sarbjeet Singh},
  keywords = {Computer vision, Deep convolutional neural network, Object detection, Deep learning, Conventional methods},
  abstract = {In the realm of computer vision, Deep Convolutional Neural Networks (DCNNs) have demonstrated excellent performance. Video Processing, Object Detection, Image Segmentation, Image Classification, Speech Recognition and Natural Language Processing are some of the application areas of CNN. Object detection is the most crucial and challenging task of computer vision. It has numerous applications in the field of security, military, transportation and medical sciences. In this review, object detection and its different aspects have been covered in detail. With the gradual increase in the evolution of deep learning algorithms for detecting objects, a significant improvement in the performance of object detection models has been observed. However, this does not imply that the conventional object detection methods, which had been evolving for decades prior to the emergence of deep learning, had become outdated. There are some cases where conventional methods with global features are superior choice. This review paper starts with a quick overview of object detection followed by object detection frameworks, backbone convolutional neural network, and an overview of common datasets along with the evaluation metrics. Object detection problems and applications are also studied in detail. Some future research challenges in designing deep neural networks are discussed. Lastly, the performance of object detection models on PASCAL VOC and MS COCO datasets is compared and conclusions are drawn.}
}

@article{object_detection_metrics,
  author         = {Padilla, Rafael and Passos, Wesley L. and Dias, Thadeu L. B. and Netto, Sergio L. and da Silva, Eduardo A. B.},
  title          = {A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit},
  journal        = {Electronics},
  volume         = {10},
  year           = {2021},
  number         = {3},
  article-number = {279},
  url            = {https://www.mdpi.com/2079-9292/10/3/279},
  issn           = {2079-9292},
  abstract       = {Recent outstanding results of supervised object detection in competitions and challenges are often associated with specific metrics and datasets. The evaluation of such methods applied in different contexts have increased the demand for annotated datasets. Annotation tools represent the location and size of objects in distinct formats, leading to a lack of consensus on the representation. Such a scenario often complicates the comparison of object detection methods. This work alleviates this problem along the following lines: (i) It provides an overview of the most relevant evaluation methods used in object detection competitions, highlighting their peculiarities, differences, and advantages; (ii) it examines the most used annotation formats, showing how different implementations may influence the assessment results; and (iii) it provides a novel open-source toolkit supporting different annotation formats and 15 performance metrics, making it easy for researchers to evaluate the performance of their detection algorithms in most known datasets. In addition, this work proposes a new metric, also included in the toolkit, for evaluating object detection in videos that is based on the spatio-temporal overlap between the ground-truth and detected bounding boxes.},
  doi            = {10.3390/electronics10030279}
}


@misc{object_detection_survey,
  title         = {Object Detection in 20 Years: A Survey},
  author        = {Zhengxia Zou and Keyan Chen and Zhenwei Shi and Yuhong Guo and Jieping Ye},
  year          = {2023},
  eprint        = {1905.05055},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1905.05055}
}

@article{precision_recall,
  title   = {Recall, precision and average precision},
  author  = {Zhu, Mu},
  journal = {Department of Statistics and Actuarial Science, University of Waterloo, Waterloo},
  volume  = {2},
  number  = {30},
  pages   = {6},
  year    = {2004}
}

@misc{lrp_error,
  title         = {One Metric to Measure them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks},
  author        = {Kemal Oksuz and Baris Can Cam and Sinan Kalkan and Emre Akbas},
  year          = {2021},
  eprint        = {2011.10772},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2011.10772}
}

@article{eclair,
  title   = {$\backslash$'Eclair--Extracting Content and Layout with Integrated Reading Order for Documents},
  author  = {Karmanov, Ilia and Deshmukh, Amala Sanjay and Voegtle, Lukas and Fischer, Philipp and Chumachenko, Kateryna and Roman, Timo and Sepp{\"a}nen, Jarno and Parmar, Jupinder and Jennings, Joseph and Tao, Andrew and others},
  journal = {arXiv preprint arXiv:2502.04223},
  year    = {2025}
}

@misc{maximize_f1,
  title         = {Thresholding Classifiers to Maximize F1 Score},
  author        = {Zachary Chase Lipton and Charles Elkan and Balakrishnan Narayanaswamy},
  year          = {2014},
  eprint        = {1402.1892},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1402.1892}
}

@inproceedings{coco,
  title        = {Microsoft coco: Common objects in context},
  author       = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle    = {European conference on computer vision},
  pages        = {740--755},
  year         = {2014},
  organization = {Springer}
}

@article{faster-coco-eval,
  title  = {{Faster-COCO-Eval}: Faster and Enhanced COCO Evaluation Library},
  author = {MiXaiLL76},
  year   = {2024}
}