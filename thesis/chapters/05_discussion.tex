% !Tex root = ../main.tex

\chapter{Discussion}\label{chapter:discussion}
We reflect on and interpret the results of our experiments with a primary focus on the suitability of established datasets and metrics for the oncology domain. We connect these findings to the research questions defined in \autoref{section:objectives} and identify the most promising methods across the different document segmentation tasks.

\section{Document Layout Analysis Evaluation}
% Why is list performance subpar?
A critical finding in the \gls{dla} evaluation was the inability of Document AI and LlamaParse, the two commercial cloud-based \gls{dp} services, to correctly identify ``list'' elements. These failures and the overall low F1 scores across this category can likely be attributed to the visual similarities between list items and paragraphs. The parsing output shows that models often misclassify list items as paragraphs, interpreting the bullet points as additional characters within the element's text. The variability in the styling of bullet point markers is likely further complicating this distinction. This misclassification obscures the relationships between the list items, leading to the loss of structural information. This is especially problematic for the parsing of oncology guidelines, as lists often contain causal relationships such as the different steps of a specific therapy process.
%

% VLMs for DLA
We found that, while Docling's and MinerU's pipeline-based approaches produced similar results, there was a significant gap between the performance of their \gls{vlm}-based counterparts. MinerU 2.5 \gls{vlm} demonstrated that domain-specific \glspl{vlm} can outperform general-purpose cloud-based models such as Gemini 2.5 Flash on \gls{dla} tasks. In contrast, Granite Docling's subpar performance suggests that its compact 256-million parameter architecture lacks the representational capacity to generalize to the scientific layouts in PubLayNet. While lightweight models offer efficiency benefits, our results therefore highlight the need for a minimum level of model complexity to perform robust \gls{dla} on scientific layouts.
%

% LlamaParse is not good for DLA despite being a closed-source cloud service
Besides Granite Docling, LlamaParse produced the lowest weighted F1 score. One possible factor that contributed to this performance slide is that LlamaParse occasionally returned invalid bounding boxes, including coordinates that were negative or extended beyond the page dimensions. This underperformance highlights the inherent risk of ``hallucinations'' that comes with \gls{vlm}-based \gls{dp} approaches. Besides its aforementioned inability to detect ``list'' elements, we found that LlamaParse was not able to correctly identify elements of the ``figure'' type.
While the low F1 scores in the ``figure'' category reported by multiple other implementations can largely be attributed to the merging of adjacent plots and subplots into a single element, LlamaParse's failure is of a different nature. Specifically, it stems from a mismatch in recognized element types, with LlamaParse critically missing a category for figures and images, making the recognition of these elements impossible. LlamaParse's unpredictability and overall disappointing performance in our \gls{dla} evaluation contrast its marketing and raise questions regarding its overall efficacy.
%

% Why do we see differences when we increase the IoU threshold?
The observed disparities between the reported F1@50 and F1@50:95 highlight the difference in precision between the bounding boxes of different implementations. Increasing the \gls{iou} value yields a stricter metric, requiring predictions to be more closely aligned with the \glspl{gtbb} in order to be counted as a \gls{tp}.

% Why do scores for title fall the most?
We observed that the most pronounced decrease in F1 scores occurred within the ``title'' category. This sensitivity is inherent to the characteristics of the title elements' bounding boxes. As titles typically possess the smallest spatial area, minor deviations in the \gls{dtbb} are enough to result in a significant drop in \gls{iou}. On the other hand, larger elements, such as tables, benefit from an increased absolute pixel tolerance which results in F1 scores remaining stable as the \gls{iou} threshold increases.

% Why does MinerU 2.5 VLM fall the most of all approaches?
Across the evaluated \gls{dp} approaches, MinerU 2.5 \gls{vlm} experienced the largest decline when transitioning to the F1@50:95. This suggests that while the model excels at predicting correct labels and finding bounding boxes for each element, the precision of these bounding boxes lags behind that of other implementations. The opposite can be said about Granite Docling, which showed the smallest performance decrease, albeit as the model with the lowest weighted F1 score.
% Who is the best at DLA?
For a \gls{rag}-based knowledge assistant that provides visual source attribution, a high F1@50:95 is critical, as inaccurate bounding boxes might result in the wrong text being highlighted for the user. This reduces overall transparency and trust in the system. Therefore, although MinerU 2.5 \gls{vlm} produced the highest weighted F1@50 score, Docling with its specialized Heron \gls{dla} model is preferable for our application.
%

\paragraph{RQ1:} We analyze how the challenges introduced by oncology guidelines are reflected specifically in the PubLayNet dataset. We find that PubLayNet incorporates many of the identified attributes of oncology guidelines, as the entirety of the dataset consists of medical journal literature with a text-centric layout in both double and single column variations. Additionally, the dataset contains large, complex tables and figures which is consistent with the \glspl{cpg}. However, PubLayNet only consists of vertical documents, missing the variations in page orientations we identified in \autoref{section:oncology_guidelines}. Furthermore, a major drawback of PubLayNet and other established \gls{dla} datasets is the sole use of single-page \gls{pdf} documents, making evaluations of multi-page structural elements, such as the long tables found in many oncology guidelines, impossible. Lastly, PubLayNet consists of scanned \gls{pdf} documents, while \glspl{cpg} are usually born-digital documents. This could lead to \gls{dp} systems performing slightly worse on PubLayNet as they are not able to extract programmatic information from the document.

\paragraph{RQ2:} We conclude that the F1 score is a suitable metric for measuring the \gls{dla} performance of the evaluated \gls{dp} systems. Due to its invariance towards confidence scores it enables the comparison of hard predictors, which is a critical aspect in the evolving landscape of \gls{dp} implementations. Many of the evaluated implementations fall into this category such as Docling, which does not include the element-level confidence scores in its final output. Most critically this enables the evaluation of the \gls{dla} performance for end-to-end \glspl{vlm}, where reported confidence scores are often unreliable \autocite{vlm_unreliable_confidence}. Through the F1 score we are able to measure the \gls{dla} performance of the entire \gls{dp} system, taking additional post-processing steps into account that are performed after the \gls{dla} stage. However, we note that reporting the F1 score at a single fixed \gls{iou} threshold is insufficient to facilitate an in-depth analysis of the \gls{dla} performance and might mask underlying problems of the implementation's predictions. By determining the F1 scores at multiple \gls{iou} thresholds, we are able to draw conclusions about both the overall performance of the model as well as the accuracy of its bounding boxes. Lastly, reporting the metric individually for each category aids with identifying weak points of the \gls{dp} approach, specifically for less common element types like ``figure'' or ``list''.

\section{Content Extraction Evaluation}
Our results show that \glspl{vlm} have the potential to outperform pipeline-based approaches specifically for content extraction. MinerU 2.5 \gls{vlm} dominating the benchmark and Gemini 2.5 Flash reaching a higher score than Google's specialized \gls{dp} service Document AI LayoutParser confirmed this finding. However, Granite Docling's underwhelming performance again marks the critical need for a minimum amount of parameters for \glspl{vlm} to be applicable in \gls{dp} applications. The strong performance of Unstructured.io and Docling on the structure-only \gls{teds} shows the positive effect of including a specialized module for table structure recognition in pipeline-based approaches. In combination with their relatively weak performance on the \gls{teds} metric and the normalized edit distance, we conclude that these approaches have outstanding capabilities for understanding the structure of the tables but are ultimately constrained by their lacking text extraction.

We also observe that open-source implementations are able to outperform proprietary solutions on content extraction, as the closed-source LlamaParse finished last, struggling with correctly identifying text content and reading order. Even Document AI, which we included to represent the gold standard of \gls{dp} implementations, was not able to match the overall performance of multiple open-source approaches.

Through the assessment of the content extraction performance of the \gls{dp} approaches, we are able to identify the multiple key findings for our research questions.

\paragraph{RQ1:} We evaluated the alignment between the document attributes of OmniDocBench and the identified characteristics of the oncology guidelines. Although only approximately 20 percent of the English scientific literature subset is comprised of medical literature, with the remainder covering other domains such as engineering, chemistry, mathematics, and computer science, we argue that the achieved results remain highly applicable to the oncology guidelines domain. This is because the document pages adhere to the same text-centric single and double-column format as the \glspl{cpg}. In addition, OmniDocBench extends this structural variety by including triple-column layouts and irregular pages with sparse texts or figures. This additional variety further examines the stability of the \gls{dp} approaches' content extraction capabilities. The inclusion of a large amount of complex tables in both vertical and horizontal orientations also aligns well with the elements found in oncology guidelines. However, we note that OmniDocBench suffers from the same inherent limitations as PubLayNet. Its reliance on single-page \gls{pdf} documents prevents the evaluation of multi-page structural elements, while the absence of digital-born documents leads to less favorable conditions than the oncology guidelines. Finally, while we acknowledge the limited scale of the evaluation subset, we argue that the high density of complex layouts and elements within these cases provides a rigorous stress test that measures the ability of the approaches to adapt to challenging compositions.

\paragraph{RQ2:} We find that evaluating the efficacy of the \gls{dp} implementations' content extraction capabilities requires an evaluation across multiple dimensions. Relying on a single metric for this evaluation would risk masking critical failures in the extraction process, while only giving limited insights into the element-specific extraction capabilities of the implementation.

To evaluate the accuracy of the implementation's text extraction we adopt the normalized edit distance. In contrast to n-gram based metrics which are also frequently used for this evaluation, such as BLEU \autocite{bleu}, ROUGE \autocite{rouge}, and METEOR \autocite{meteor}, the normalized edit distance measures the character-level accuracy of the extracted text while strictly penalizing incorrect word order. We argue that in the context of our application domain, where character-level precision and correct word ordering are crucial to maintaining the scientific integrity of the medical recommendations, the normalized edit distance is the superior metric for measuring the quality of the extracted content.

While the normalized edit distance quantifies the accuracy of text within individual elements, it fails to assess the overall structure of the document. A parser might extract the content of every detected element perfectly, but fail to arrange them in the correct order by missing the double-column layout of the document. The reading order edit distance fills this gap in the evaluation methodology by measuring the system's ability to preserve the logical sequence of the structural elements.

Lastly, we integrate the \gls{teds} metric into our evaluation, as providing accurate content extraction for complex table elements is a crucial requirement for our application. In addition, we are able to analyze the models table structure understanding in isolation from its text extraction capabilities by reporting the metric's structure-only version.

\section{Chunking Evaluation}
Our evaluation of the chunking configurations revealed a consistent, overarching trade-off between token-wise precision and recall. As the maximum chunk size increased, recall generally improved while precision continuously degraded. The underlying explanation for this effect is straightforward. Larger chunks incorporate more tokens and are naturally more likely to contain a relevant text excerpt. However, because the absolute number of relevant tokens remains constant, expanding the chunk size also introduces additional noise, decreasing the overall ratio of relevant excerpts.

We find that there is a definitive upper limit for the positive correlation between chunk size and recall. For fixed-size chunking we observed that increasing $N$ above 512 lead to a decrease in overall recall. We hypothesize that extracting excessively large chunks of text without regarding the structure of the document introduces substantial noise. Consequently, the retriever is not able to locate the relevant information within the chunks, causing a decrease in retrieval performance. Additionally, our results show that including an overlap is less effective for recursive character chunking, leading to a decrease in precision with very little change in the recall value. For the general \gls{qa} dataset, we even find that including an overlap lead to a decrease in recall for the recursive character chunking. We therefore conclude that when chunks follow the structure of the document overlap mainly introduces additional noise and should therefore only be included for chunking strategies that produce window passages.

Another crucial finding of our evaluation is the stagnation of the recall scores across different hierarchical chunking configurations. Unlike the other evaluated strategies, increasing the maximum chunk size did not lead to a change in recall for hierarchical chunking. Hereby, we note that the perceived increased performance of the strategy on the general \gls{qa} datasets is not indicative for the performance of the hierarchical chunker itself. These datasets do not have any inherent document structure, which leads to the hierarchical chunker having to fall back exclusively on the recursive character chunker. Therefore, the results achieved by the hierarchical chunker mirror those of the recursive character chunker.

We hypothesize that this observed stagnation effect is caused by the headings which are prepended to the chunk's content. If multiple chunks begin with the same text their resulting vector embeddings might become overly similar, preventing the retriever from successfully identifying the most relevant excerpts. However, because this strategy initially showed potential for yielding chunks with a high token-wise precision, we believe that exploring techniques to resolve this conflict represents a valuable direction for future research.

We attribute the crossover effect identified in breakpoint-based semantic chunking to the interaction between the similarity threshold and the maximum chunk size. Higher thresholds require more significant topical shifts, resulting in larger, semantically distinct chunks. Conversely, smaller thresholds produce more granular chunks. As the maximum chunk size increases, chunks from lower thresholds need to be merged to satisfy the minimum chunk length requirement. This merging process likely decreases semantic coherence inside the merged chunk. Therefore, we observe that configurations with higher thresholds tend to perform better as maximum chunk size increases. Regardless of the chosen parameters, our findings align with those of \textcite{semantic_chunking}, finding that breakpoint-based semantic chunking does not justify its substantial computational overhead, as it does not yield any notable performance improvements compared to recursive character chunking.

Ultimately, while the evaluated chunking strategies demonstrated more comparable performance levels than initially expected, our findings identify recursive character chunking as the most robust solution for our application. Specifically, the configuration with a maximum chunk size of 512 tokens and zero overlap achieved high recall alongside promising precision values. Our overarching conclusion regarding the results of our evaluation is that tuning the parameters can be more crucial than the selection of the strategy itself.

Based on the results of our evaluation, we were able to identify multiple key findings regarding which metrics served useful for evaluating chunking configurations.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{images/iou_example}
  \caption[Visual example for the token-wise \gls{iou} of two different chunking strategies]{Visual example for the token-wise \gls{iou} of two different chunking strategies. For a given query and chunking strategy, the tokens from the blue area are retrieved, while the tokens from the yellow area are needed to answer the question. The set of correctly retrieved tokens is denoted by the red area. We see that despite the chunk from chunking strategy 2 providing the entirety of the needed information needed to answer a question, the \gls{iou} score is the same as strategy 1 due to the lower precision of strategy 2.}\label{fig:iou_example}
\end{figure}

\paragraph{RQ2:} Completing our evaluation of suitable metrics, we identify the token-wise recall as the most important metric for measuring the efficacy of the chunking strategies. If the retrieval concludes with low recall, the generator will not have access to critical information needed to answer a query. This leads to inaccurate and ill informed answers from the system that could potentially lead to dubious medical recommendations. Simultaneously, token-wise precision remains a critical aspect of our analysis. Low precision indicates an increased ratio of distracting information included in the chunks, which can subsequently deteriorate the performance of the generator \autocite{including_distractions_rag,llm_context_distractions}. Additionally, low precision is detrimental to the accuracy of visual source attribution. If only a small amount of the highlighted text is relevant to the query, additional cognitive strain is placed on the user who has to manually review the content of the bloated bounding boxes. We find that the evaluation of different chunking strategies needs to take both metrics into account, albeit with a focus on maintaining a high token-wise recall.

We note that the inherent difficulty of the queries and the varying amount of required context to answer them currently suppress the achieved evaluation scores. We hypothesize that with further retrieval optimization, both precision and recall could be drastically improved. Therefore, we find that $\text{precision}_\Omega$ proves to be an important indicator for the potential precision of the chunking configuration by establishing an upper bound for the retriever's precision. This metric essentially measures how accurately the created chunks align with the semantical structure of the document, while controlling for the performance of the retriever.

On the other hand, our results indicate that the token-wise \gls{iou} is not a suitable metric for this evaluation, as it is heavily biased towards the achieved precision score. Fundamentally, the \gls{iou} does not differentiate between the two bounding boxes when determining their union. For the token-wise \gls{iou}, this means that retrieving an irrelevant token is penalized the same as not retrieving a relevant token. We highlight the issues of this metric through a visual example in \autoref{fig:iou_example}. As recall is the most critical aspect of our evaluation, valuing it the same as precision does not reflect the nature of the retrieval process. While the generator is able to tolerate some noise \autocite{including_distractions_rag}, missing important information that is needed to answer the query can not be made up for in subsequent stages.

\section{Document Segmentation Enhancements}
Addressing \textbf{RQ3}, which investigates how document segmentation methods can be adapted and expanded on to fulfill the visual source attribution requirements of a \gls{rag} based knowledge assistant, we integrated two distinct enhancements into our document segmentation pipeline. Firstly, we enhanced the relation integration of the \gls{dp} implementations by implementing additional post-processing steps. Specifically, we filter out extraneous elements, infer the document's structural hierarchy based on the reading order, and determine span-level bounding boxes. These additions optimize the document tree, enabling the use of structure-dependent chunking strategies such as hierarchical chunking while ensuring that downstream chunking output only contains relevant content. Our second contribution entails the proposition of a novel token-based methodology for the chunking process. This approach enables traceability for every constituent token even when employing chunking strategies that do not inherently respect the structure of the document. Combined with the span-level bounding boxes introduced in the \gls{dp} post-processing, our pipeline successfully generates chunk bounding boxes at a higher level of granularity than previous implementations. However, it is important to note that the precision of the produced bounding boxes has so far only been verified through qualitative, small-scale evaluations. Therefore, future work should incorporate a more rigorous quantitative evaluation to validate the robustness of this novel approach.