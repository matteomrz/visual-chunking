% !Tex root = ../main.tex

\chapter{Discussion}\label{chapter:discussion}
We reflect on and interpret the results of our experiments with a primary focus on the suitability of established datasets and metrics for the oncology domain. We connect these findings to the research questions defined in \autoref{section:objectives} and identify the most promising methods across the different document segmentation tasks.

\section{Document Parsing Evaluation}

\subsection{Document Layout Analysis Evaluation}
% Why is list performance subpar?
A critical finding in the \gls{dla} evaluation was the inability of Document AI and LlamaParse, the two commercial cloud-based \gls{dp} services, to correctly identify ``list'' elements. These failures and the overall low F1 scores across this category can likely be attributed to the visual similarities between list items and paragraphs. The parsing output shows that models often misclassify list items as paragraphs, interpreting the bullet points as additional characters within the elements text. The variability in the styling of bullet point markers is likely further complicating this distinction. This misclassification obscures the relationships between the list items, leading to the loss of structural information. This is especially problematic for the parsing of oncology guidelines, as lists often contain causal relationships, such as the different steps of a specific therapy process.
%

% VLMs for DLA
We found that, while Docling's and MinerU's pipeline-based approaches produced similar results, there was a significant gap between the performance of their \gls{vlm}-based counterparts. MinerU 2.5 \gls{vlm} demonstrated that domain-specific \glspl{vlm} can outperform general-purpose cloud-based models such as Gemini 2.5 Flash on \gls{dla} tasks. In contrast, Granite Docling's subpar performance suggests that its compact 256-million parameter architecture lacks the representational capacity to generalize to the scientific layouts in PubLayNet. While lightweight models offer efficiency benefits, our results therefore highlight the need for a minimum level of model complexity to perform robust \gls{dla} on scientific layouts.
%

% LlamaParse is not good for DLA despite being a closed-source cloud service
Besides Granite Docling, LlamaParse produced the lowest weighted F1 score. One possible factor that attributed to this performance slide is that LlamaParse occasionally returned invalid bounding boxes, including coordinates that were negative or extended beyond the page dimensions. This underperformance highlights the inherent risk of ``hallucinations'' that comes with \gls{vlm}-based \gls{dp} approaches. Besides its aforementioned inability to detect ``list'' elements, we found that LlamaParse was not able to correctly identify elements of the ``figure'' type.
While the low F1 scores in the ``figure'' category reported by multiple other implementations can largely be attributed to the merging of adjacent plots and subplots into a single element, LlamaParse's failure is of a different nature. Specifically, it stems from a mismatch in recognized element types, with LlamaParse critically missing a category for figures and images, making the recognition of these elements impossible. LlamaParse's unpredictability and overall disappointing performance in our \gls{dla} evaluation contrast its marketing and raise questions regarding its overall efficacy.
%

% Why do we see differences when we increase the IoU threshold?
The observed disparities between the reported F1@50 and F1@50:95 highlight the difference in precision between the bounding boxes of different implementations. Increasing the \gls{iou} value yields a stricter metric, requiring predictions to be more closely aligned with the \glspl{gtbb} in order to be counted as a \gls{tp}.

% Why do scores for title fall the most?
We observed that the most pronounced decrease in F1 scores occurred within the ``title'' category. This sensitivity is inherent to the characteristics of the title elements' bounding boxes. As titles typically possess the smallest spatial area, minor deviations in the \gls{dtbb} are enough to result in a significant drop in \gls{iou}. On the other hand, larger elements, such as tables, benefit from an increased absolute pixel tolerance which results in F1 scores remaining stable as the \gls{iou} threshold increases.

% Why does MinerU 2.5 VLM fall the most of all approaches?
Across the evaluated \gls{dp} approaches, MinerU 2.5 \gls{vlm} experienced the largest decline when transitioning to the F1@50:95. This suggests that while the model excels at predicting correct labels and finding bounding boxes for each element, the precision of these bounding boxes lags behind that of other implementations. The opposite can be said about Granite Docling, which showed the smallest performance decrease, albeit as the model with the lowest weighted F1 score.
% Who is the best at DLA?
For a \gls{rag}-based knowledge assistant that provides visual source attribution, a high F1@50:95 is critical, as inaccurate bounding boxes might result in the wrong text being highlighted for the user. This reduces overall transparency and trust in the system. Therefore, although MinerU 2.5 \gls{vlm} produced the highest weighted F1@50 score, Docling with its specialized Heron \gls{dla} model is preferable for our application.
%

% % RQ1: What attributes of oncology guidelines does PubLayNet incorporate
% As previously mentioned, PubLayNet combines many important characteristics of oncology guideline documents. PubLayNet is constructed exclusively out of medical journal literature, matching the domain of the oncology guidelines. The annotations in the dataset reflect the text-heavy layout of the \glspl{cpg}, while being available as both single and double column layouts. In addition, the dataset contains large complicated tables and figures which is consistent with the oncology guidelines.

% We find that PubLayNet incorporates many of the identified attributes of oncology guidelines. As the entirety of the dataset consists of medical journal literature, the application domain of the documents is very similar. As such, documents from PubLayNet follow the same text-centric layout in both double and single column variations. In addition, the documents contain large complicated tables and figures which is consistent with the oncology guidelines. PubLayNet only consists of vertical documents, missing the variations in page orientations of oncology guidelines. One major drawback of current \gls{dp} datasets is that they are usually solely comprised of single-page \gls{pdf} documents. This makes evaluations including multi-page structural elements, such as the tables in many oncology guidelines, impossible.
% %

% % RQ2: Why is the metric good?
% The F1 score is a suitable metric for measuring the \gls{dla} capabilities of the \gls{dp} approaches. Due to its invariance towards confidence scores it enables the comparison of \glspl{vlm} and other hard predictors, which is a critical aspect in the evolving landscape of \gls{dp} implementations. In contrast to previous evaluations we are therefore able to measure the \gls{dla} performance of an entire \gls{dp} system, evaluating the quality of the final bounding box output including eventual post-processing steps. By evaluating the metric at multiple \gls{iou} values, we are both able to draw conclusions about the overall accuracy of the \gls{dla} process as well as the quality of the predicted bounding boxes. Additionally, reporting the metric individually for each category aids with identifying weak points of the \gls{dp} approach, especially for less common element types.
% %

\paragraph{RQ1:} We analyze how the challenges introduced by oncology guidelines are reflected specifically in the PubLayNet dataset. We find that PubLayNet incorporates many of the identified attributes of oncology guidelines, as the entirety of the dataset consists of medical journal literature with a text-centric layout in both double and single column variations. Additionally, the dataset contains large, complex tables and figures which is consistent with the \glspl{cpg}. However, PubLayNet only consists of vertical documents, missing the variations in page orientations we identified in \autoref{section:oncology_guidelines}. Furthermore, a major drawback of PubLayNet and other established \gls{dla} datasets is the sole use of single-page \gls{pdf} documents, making evaluations of multi-page structural elements, such as the long tables found in many oncology guidelines, impossible. Lastly, PubLayNet consists of scanned \gls{pdf} documents, while \glspl{cpg} are usually born-digital documents. This could lead to \gls{dp} systems performing slightly worse on PubLayNet as they are not able to extract programmatic information from the document.

\paragraph{RQ2:} We conclude that the F1 score is a suitable metric for measuring the \gls{dla} performance of the evaluated \gls{dp} systems. Due to its invariance towards confidence scores it enables the comparison of hard predictors, which is a critical aspect in the evolving landscape of \gls{dp} implementations. Many of the evaluated implementations fall into this category such as Docling, which does not include the element-level confidence scores in its final output. Most critically this enables the evaluation of the \gls{dla} performance for end-to-end \glspl{vlm}, where reported confidence scores are often unreliable \autocite{vlm_unreliable_confidence}. Through the F1 score we are able to measure the \gls{dla} performance of the entire \gls{dp} system, taking additional post-processing steps into account that are performed after the \gls{dla} stage. However, we note that reporting the F1 score at a single fixed \gls{iou} threshold is insufficient to facilitate an in-depth analysis of the \gls{dla} performance and might mask underlying problems of the implementation's predictions. By determining the F1 scores at multiple \gls{iou} thresholds, we are able to draw conclusions about both the overall performance of the model as well as the accuracy of its bounding boxes. Lastly, reporting the metric individually for each category aids with identifying weak points of the \gls{dp} approach, specifically for less common element types like ``figure'' or ``list''.

\subsection{Content Extraction Evaluation}
Our results show that \glspl{vlm} have the potential to outperform pipeline-based approaches specifically for content extraction. MinerU 2.5 \gls{vlm} dominating the benchmark and Gemini 2.5 Flash reaching a higher score than Google's specialized \gls{dp} service Document AI LayoutParser confirmed this finding. However, Granite Docling's underwhelming performance again marks the critical need for a minimum amount of parameters for \glspl{vlm} to be applicable in \gls{dp} applications. The strong performance of Unstructured.io and Docling on the structure-only \gls{teds} shows the positive effect of including a specialized module for table structure recognition in pipeline-based approaches. In combination with their relatively weak performance on the \gls{teds} metric and the normalized edit distance, we conclude that these approaches have outstanding capabilities for understanding the structure of the tables but are ultimately constrained by their lacking text extraction.

We also observe that open-source implementations are able to outperform proprietary solutions on content extraction, as the closed-source LlamaParse finished last, struggling with correctly identifying text content and reading order. Even Document AI, which we included to represent the gold standard of \gls{dp} implementations, was not able to match the overall performance of multiple open-source approaches.

Through the assessment of the content extraction performance of the \gls{dp} approaches, we are able to identify the multiple key findings for our research questions.

\paragraph{RQ1:} We evaluated the alignment between the document attributes of OmniDocBench and the identified characteristics of the oncology guidelines. Although only approximately 20\% of the English scientific literature subset is comprised of medical literature, with the remainder covering other domains such as engineering, chemistry, mathematics, and computer science, we argue that the achieved results remain highly applicable to the oncology guidelines domain. This is because the document pages adhere to the same text-centric single and double-column format as the \glspl{cpg}. In addition, OmniDocBench extends this structural variety by including triple-column layouts and irregular pages with sparse texts or figures. This additional variety further examines the stability of the \gls{dp} approaches' content extraction capabilities. The inclusion of a large amount of complex tables in both vertical and horizontal orientations also aligns well with the elements found in oncology guidelines. However, we note that OmniDocBench suffers from the same inherent limitations as PubLayNet. Its reliance on single-page \gls{pdf} documents prevents the evaluation of multi-page structural elements, while the absence of digital-born documents leads to less favorable conditions than the oncology guidelines. Finally, while we acknowledge the limited scale of the evaluation subset, we argue that the high density of complex layouts and elements within these cases provides a rigorous stress test that measures the ability of the approaches to adapt to challenging compositions.

\paragraph{RQ2:} We find that evaluating the efficacy of the \gls{dp} implementations' content extraction capabilities requires an evaluation across multiple dimensions. Relying on a single metric for this evaluation would risk masking critical failures in the extraction process, while only giving limited insights into the element-specific extraction capabilities of the implementation.

To evaluate the accuracy of the implementation's text extraction we adopt the normalized edit distance. In contrast to n-gram based metrics which are also frequently used for this evaluation, such as BLEU \autocite{bleu}, ROUGE \autocite{rouge}, and METEOR \autocite{meteor}, the normalized edit distance measures the character-level accuracy of the extracted text while strictly penalizing incorrect word order. We argue that in the context of our application domain, where character-level precision and correct word ordering are crucial to maintaining the scientific integrity of the medical recommendations, the normalized edit distance is the superior metric for measuring the quality of the extracted content.

While the normalized edit distance quantifies the accuracy of text within individual elements, it fails to assess the overall structure of the document. A parser might extract the content of every detected element perfectly, but fail to arrange them in the correct order by missing the double-column layout of the document. The reading order edit distance fills this gap in the evaluation methodology by measuring the system's ability to preserve the logical sequence of the structural elements.

Lastly, we integrate the \gls{teds} metric into our evaluation, as providing accurate content extraction for complex table elements is a crucial requirement for our application. In addition, we are able to analyze the models table structure understanding in isolation from its text extraction capabilities by reporting the metric's structure-only version.

\section{Chunking Evaluation}
% The evaluation results are close to each other

% Based on $\text{Precision}_\Omega$, we see that strategies that follow more complex approaches produce chunks that are inherently better aligned with the underlying structure of the document. We see that these strategies include fewer unneeded tokens.

% For research question 3:
% We proposed a novel solution aimed at enabling traceability of the chunks tokens to its constituent ParsingResult nodes.

% \section{}


% \subsection{draftrq}

% We can answer RQ1 with the following:

% % PUBLAYNET
% We find that for \gls{dla} analysis PubLayNet incorporates many of the identified attributes of oncology guidelines. As the entirety of the dataset consists of medical journal literature, the application domain of the documents is very similar. As such, documents from PubLayNet incorporated the same text-centric layout in both double and single column variations. In addition, the documents contain large complicated tables and figures which is consistent with the oncology guidelines. PubLayNet only consists of vertical documents, missing the variations in page orientations of oncology guidelines. One major drawback of current \gls{dp} datasets is that they are usually solely comprised of single-page \gls{pdf} documents. This makes evaluations including multi-page structural elements, such as the tables in many oncology guidelines, impossible.

% We can therefore answer RQ2 with the following:

% % DLA METRIC
% The F1 score is a suitable metric for measuring the \gls{dla} capabilities of the \gls{dp} approaches. Due to its invariance towards confidence scores it enables the comparison of \glspl{vlm} and other hard predictors, which is a critical aspect in the evolving landscape of \gls{dp} implementations. In contrast to previous evaluations we are therefore able to measure the \gls{dla} performance of an entire \gls{dp} system, evaluating the quality of the final bounding box output including eventual post-processing steps. By evaluating the metric at multiple \gls{iou} values, we are both able to draw conclusions about the overall accuracy of the \gls{dla} process as well as the quality of the predicted bounding boxes. Additionally, reporting the metric individually for each category aids with identifying weak points of the \gls{dp} approach, especially for less common element types.

% % DP METRIC
% The selected metrics allow the analysis of the content extraction performance of \gls{dp} systems across multiple dimensions. Being able to analyze the table structure recognition separately using $TEDS-S$ proofs valuable for identifying which implementations are good at understanding tables, while controlling for the text recognition capabilities of the models. The normalized edit distance proofs useful as an analogy for the average amount of mistakes that the implementation made regarding the text understanding. Reading order hereby measures how well the implementations were able to adapt to the layout of the documents, understanding the structure of multi-column layouts.

% We can answer RQ3 with the following:

% IMPROVING THE SYSTEM



% These findings also support the principle of multi-stage \gls{vlm}-based approaches being able to perform better on individual tasks than single-stage \gls{vlm}

% There are multiple explanations for the zero-scores reported by Document AI and LlamaParse. LlamaParse does not have a figure or image type, which makes it impossible for the system to extract this category. As for list, the most likely explanation is that lists are often very similar to texts visually. So often the bullet point was interpreted as an extra character leading to Document AI classifying the output as text. This also explains the drop in performance for the list type across the other implementations.

% LlamaParse's bad performance can also be explained through its poor bounding boxes, sometimes even yielding invalid bounding boxes with negative coordinates. Many implementations struggled on the figure category as often figures that are placed next to each other were recognized as a single figure.\ \glspl{vlm} seem especially vulnerable to this effect. Granite Docling's performance suggests that its extremely compact 256M parameter architecture can not adapt well to the challenges of this dataset. 

%The change in hierarchy after increasing the iou value suggests that while the MinerU systems are generally better at predicting bounding boxes with an acceptable accuracy, the correct bounding boxes produced by Docling are more accurate to the ground truth. The sharp drop in title when increasing the iou value compared to other categories can be explained by the smaller average area of the bounding boxes for titles, such that there is less total error possible as the iou value increases. For large elements such as tables, even at an iou of 95\% there is still some space for errors. MinerU 2.5 VLM's performance drop suggests that, as an allrounder model that handles multiple tasks, the quality of its output for \gls{dla} may be subpar compared to the pipeline-based approach.