% !Tex root = ../main.tex

\chapter{Discussion}\label{chapter:discussion}
We reflect on and interpret the results of our experiments with a primary focus on the suitability of established datasets and metrics for the oncology domain. We connect these findings to the research questions defined in \autoref{section:objectives} and identify the most promising methods across the different document segmentation tasks.

\section{Document Layout Analysis Evaluation}
% Why is list performance subpar?
A critical finding in the \gls{dla} evaluation was the inability of Document AI and LlamaParse, the two commercial cloud-based \gls{dp} services, to correctly identify ``list'' elements. These failures and the overall low F1 scores across this category can likely be attributed to the visual similarities between list items and paragraphs. The parsing output shows that models often misclassify list items as paragraphs, interpreting the bullet points as additional characters within the elements text. The variability in the styling of bullet point markers is likely further complicating this distinction. This misclassification obscures the relationships between the list items, leading to the loss of structural information. This is especially problematic for the parsing of oncology guidelines, as lists often contain causal relationships, such as the different steps of a specific therapy process.
%

% VLMs for DLA
We found that, while Docling's and MinerU's pipeline-based approaches produced similar results, there was a significant gap between the performance of their \gls{vlm}-based counterparts. MinerU 2.5 \gls{vlm} demonstrated that domain-specific \glspl{vlm} can outperform general-purpose cloud-based models such as Gemini 2.5 Flash on \gls{dla} tasks. In contrast, Granite Docling's subpar performance suggests that its compact 256-million parameter architecture lacks the representational capacity to generalize to the scientific layouts in PubLayNet. While lightweight models offer efficiency benefits, our results therefore highlight the need for a minimum level of model complexity to perform robust \gls{dla} on scientific layouts.
%

% LlamaParse is not good for DLA despite being a closed-source cloud service
Besides Granite Docling, LlamaParse produced the lowest weighted F1 score. One possible factor that attributed to this performance slide is that LlamaParse occasionally returned invalid bounding boxes, including coordinates that were negative or extended beyond the page dimensions. This underperformance highlights the inherent risk of ``hallucinations'' that comes with \gls{vlm}-based \gls{dp} approaches. Besides its aforementioned inability to detect ``list'' elements, we found that LlamaParse was not able to correctly identify elements of the ``figure'' type.
While the low F1 scores in the ``figure'' category reported by multiple other implementations can largely be attributed to the merging of adjacent plots and subplots into a single element, LlamaParse's failure is of a different nature. Specifically, it stems from a mismatch in recognized element types, with LlamaParse critically missing a category for figures and images, making the recognition of these elements impossible. LlamaParse's unpredictability and overall disappointing performance in our \gls{dla} evaluation contrast its marketing and raise questions regarding its overall efficacy.
%

% Why do we see differences when we increase the IoU threshold?
The observed disparities between the reported F1@50 and F1@50:95 highlight the difference in precision between the bounding boxes of different implementations. Increasing the \gls{iou} value yields a stricter metric, requiring predictions to be more closely aligned with the \glspl{gtbb} in order to be counted as a \gls{tp}.

% Why do scores for title fall the most?
We observed that the most pronounced decrease in F1 scores occurred within the ``title'' category. This sensitivity is inherent to the characteristics of the title elements' bounding boxes. As titles typically possess the smallest spatial area, minor deviations in the \gls{dtbb} are enough to result in a significant drop in \gls{iou}. On the other hand, larger elements, such as tables, benefit from an increased absolute pixel tolerance which results in F1 scores remaining stable as the \gls{iou} threshold increases.

% Why does MinerU 2.5 VLM fall the most of all approaches?
Across the evaluated \gls{dp} approaches, MinerU 2.5 \gls{vlm} experienced the largest decline when transitioning to the F1@50:95. This suggests that while the model excels at predicting correct labels and finding bounding boxes for each element, the precision of these bounding boxes lags behind that of other implementations. The opposite can be said about Granite Docling, which showed the smallest performance decrease, albeit as the model with the lowest weighted F1 score.
% Who is the best at DLA?
For a \gls{rag}-based knowledge assistant that provides visual source attribution, a high F1@50:95 is critical, as inaccurate bounding boxes might result in the wrong text being highlighted for the user. This reduces overall transparency and trust in the system. Therefore, although MinerU 2.5 \gls{vlm} produced the highest weighted F1@50 score, Docling with its specialized Heron \gls{dla} model is preferable for our application.
%

% % RQ1: What attributes of oncology guidelines does PubLayNet incorporate
% As previously mentioned, PubLayNet combines many important characteristics of oncology guideline documents. PubLayNet is constructed exclusively out of medical journal literature, matching the domain of the oncology guidelines. The annotations in the dataset reflect the text-heavy layout of the \glspl{cpg}, while being available as both single and double column layouts. In addition, the dataset contains large complicated tables and figures which is consistent with the oncology guidelines.

% We find that PubLayNet incorporates many of the identified attributes of oncology guidelines. As the entirety of the dataset consists of medical journal literature, the application domain of the documents is very similar. As such, documents from PubLayNet follow the same text-centric layout in both double and single column variations. In addition, the documents contain large complicated tables and figures which is consistent with the oncology guidelines. PubLayNet only consists of vertical documents, missing the variations in page orientations of oncology guidelines. One major drawback of current \gls{dp} datasets is that they are usually solely comprised of single-page \gls{pdf} documents. This makes evaluations including multi-page structural elements, such as the tables in many oncology guidelines, impossible.
% %

% % RQ2: Why is the metric good?
% The F1 score is a suitable metric for measuring the \gls{dla} capabilities of the \gls{dp} approaches. Due to its invariance towards confidence scores it enables the comparison of \glspl{vlm} and other hard predictors, which is a critical aspect in the evolving landscape of \gls{dp} implementations. In contrast to previous evaluations we are therefore able to measure the \gls{dla} performance of an entire \gls{dp} system, evaluating the quality of the final bounding box output including eventual post-processing steps. By evaluating the metric at multiple \gls{iou} values, we are both able to draw conclusions about the overall accuracy of the \gls{dla} process as well as the quality of the predicted bounding boxes. Additionally, reporting the metric individually for each category aids with identifying weak points of the \gls{dp} approach, especially for less common element types.
% %

\paragraph{RQ1:} We analyze how the challenges introduced by oncology guidelines are reflected specifically in the PubLayNet dataset. We find that PubLayNet incorporates many of the identified attributes of oncology guidelines, as the entirety of the dataset consists of medical journal literature with a text-centric layout in both double and single column variations. Additionally, the dataset contains large, complex tables and figures which is consistent with the \glspl{cpg}. However, PubLayNet only consists of vertical documents, missing the variations in page orientations we identified in \autoref{section:oncology_guidelines}. Furthermore, a major drawback of PubLayNet and other established \gls{dla} datasets is the sole use of single-page \gls{pdf} documents, making evaluations of multi-page structural elements, such as the long tables found in many oncology guidelines, impossible. Lastly, PubLayNet consists of scanned \gls{pdf} documents, while \glspl{cpg} are usually born-digital documents. This could lead to \gls{dp} systems performing slightly worse on PubLayNet as they are not able to extract programmatic information from the document.

\paragraph{RQ2:} We conclude that the F1 score is a suitable metric for measuring the \gls{dla} performance of the evaluated \gls{dp} systems. Due to its invariance towards confidence scores it enables the comparison of hard predictors, which is a critical aspect in the evolving landscape of \gls{dp} implementations. Many of the evaluated implementations fall into this category such as Docling, which does not include the element-level confidence scores in its final output. Most critically this enables the evaluation of the \gls{dla} performance for end-to-end \glspl{vlm}, where reported confidence scores are often unreliable \autocite{vlm_unreliable_confidence}. Through the F1 score we are able to measure the \gls{dla} performance of the entire \gls{dp} system, taking additional post-processing steps into account that are performed after the \gls{dla} stage. However, we note that reporting the F1 score at a single fixed \gls{iou} threshold is insufficient to facilitate an in-depth analysis of the \gls{dla} performance and might mask underlying problems of the implementation's predictions. By determining the F1 scores at multiple \gls{iou} thresholds, we are able to draw conclusions about both the overall performance of the model as well as the accuracy of its bounding boxes. Lastly, reporting the metric individually for each category aids with identifying weak points of the \gls{dp} approach, specifically for less common element types like ``figure'' or ``list''.

\section{Content Extraction Evaluation}
Our results show that \glspl{vlm} have the potential to outperform pipeline-based approaches specifically for content extraction. MinerU 2.5 \gls{vlm} dominating the benchmark and Gemini 2.5 Flash reaching a higher score than Google's specialized \gls{dp} service Document AI LayoutParser confirmed this finding. However, Granite Docling's underwhelming performance again marks the critical need for a minimum amount of parameters for \glspl{vlm} to be applicable in \gls{dp} applications. The strong performance of Unstructured.io and Docling on the structure-only \gls{teds} shows the positive effect of including a specialized module for table structure recognition in pipeline-based approaches. In combination with their relatively weak performance on the \gls{teds} metric and the normalized edit distance, we conclude that these approaches have outstanding capabilities for understanding the structure of the tables but are ultimately constrained by their lacking text extraction.

We also observe that open-source implementations are able to outperform proprietary solutions on content extraction, as the closed-source LlamaParse finished last, struggling with correctly identifying text content and reading order. Even Document AI, which we included to represent the gold standard of \gls{dp} implementations, was not able to match the overall performance of multiple open-source approaches.

Through the assessment of the content extraction performance of the \gls{dp} approaches, we are able to identify the multiple key findings for our research questions.

\paragraph{RQ1:} We evaluated the alignment between the document attributes of OmniDocBench and the identified characteristics of the oncology guidelines. Although only approximately 20\% of the English scientific literature subset is comprised of medical literature, with the remainder covering other domains such as engineering, chemistry, mathematics, and computer science, we argue that the achieved results remain highly applicable to the oncology guidelines domain. This is because the document pages adhere to the same text-centric single and double-column format as the \glspl{cpg}. In addition, OmniDocBench extends this structural variety by including triple-column layouts and irregular pages with sparse texts or figures. This additional variety further examines the stability of the \gls{dp} approaches' content extraction capabilities. The inclusion of a large amount of complex tables in both vertical and horizontal orientations also aligns well with the elements found in oncology guidelines. However, we note that OmniDocBench suffers from the same inherent limitations as PubLayNet. Its reliance on single-page \gls{pdf} documents prevents the evaluation of multi-page structural elements, while the absence of digital-born documents leads to less favorable conditions than the oncology guidelines. Finally, while we acknowledge the limited scale of the evaluation subset, we argue that the high density of complex layouts and elements within these cases provides a rigorous stress test that measures the ability of the approaches to adapt to challenging compositions.

\paragraph{RQ2:} We find that evaluating the efficacy of the \gls{dp} implementations' content extraction capabilities requires an evaluation across multiple dimensions. Relying on a single metric for this evaluation would risk masking critical failures in the extraction process, while only giving limited insights into the element-specific extraction capabilities of the implementation.

To evaluate the accuracy of the implementation's text extraction we adopt the normalized edit distance. In contrast to n-gram based metrics which are also frequently used for this evaluation, such as BLEU \autocite{bleu}, ROUGE \autocite{rouge}, and METEOR \autocite{meteor}, the normalized edit distance measures the character-level accuracy of the extracted text while strictly penalizing incorrect word order. We argue that in the context of our application domain, where character-level precision and correct word ordering are crucial to maintaining the scientific integrity of the medical recommendations, the normalized edit distance is the superior metric for measuring the quality of the extracted content.

While the normalized edit distance quantifies the accuracy of text within individual elements, it fails to assess the overall structure of the document. A parser might extract the content of every detected element perfectly, but fail to arrange them in the correct order by missing the double-column layout of the document. The reading order edit distance fills this gap in the evaluation methodology by measuring the system's ability to preserve the logical sequence of the structural elements.

Lastly, we integrate the \gls{teds} metric into our evaluation, as providing accurate content extraction for complex table elements is a crucial requirement for our application. In addition, we are able to analyze the models table structure understanding in isolation from its text extraction capabilities by reporting the metric's structure-only version.

\section{Chunking Evaluation}
Our evaluation of the chunking configurations revealed a consistent, overarching trade-off between token-wise precision and recall. As the maximum chunk size increased, recall generally improved while precision continuously degraded. The underlying explanation for this effect is straightforward. Larger chunks incorporate more tokens and are naturally more likely to contain a relevant text excerpt. However, because the absolute number of relevant tokens remains constant, expanding the chunk size also introduces additional noise, decreasing the overall ratio of relevant excerpts.

We find that there is a definitive upper limit for the positive correlation between chunk size and recall. For fixed-size chunking we observed that increasing $N$ above 512 lead to a decrease in overall recall. We hypothesize that extracting excessively large chunks of text without regarding the structure of the document introduces substantial noise. Consequently, the retriever is not able to locate the relevant information within the chunks, causing a decrease in retrieval performance. Additionally, our results show that including an overlap is less effective for recursive character chunking, leading to a decrease in precision with very little change in the recall value. We therefore conclude that when chunks follow the structure of the document overlap mainly introduces additional noise and should therefore only be included for chunking strategies that produce window passages.

Another crucial finding of our evaluation is the stagnation of the recall scores across different hierarchical chunking configurations. Unlike the other evaluated strategies, increasing the maximum chunk size did not lead to a change in recall for hierarchical chunking. We hypothesize that this effect is caused by the headings which are prepended to the chunk's content. If multiple chunks begin with the same text their resulting vector embeddings might become overly similar, preventing the retriever from successfully identifying the most relevant excerpts. However, because this strategy initially showed potential for yielding chunks with a high token-wise precision, we believe that exploring techniques to resolve this conflict represents a valuable direction for future research.

We attribute the crossover effect identified in breakpoint-based semantic chunking to the interaction between the similarity threshold and the maximum chunk size. Higher thresholds require more significant topical shifts, resulting in larger, semantically distinct chunks. Conversely, smaller thresholds produce more granular chunks. As the maximum chunk size increases, chunks from lower thresholds need to be merged to satisfy the minimum chunk length requirement. This merging process likely decreases semantic coherence inside the merged chunk. Therefore, we observe that configurations with higher thresholds tend to perform better as maximum chunk size increases. Regardless of the chosen parameters, our findings align with those of \textcite{semantic_chunking}, finding that breakpoint-based semantic chunking does not justify its substantial computational overhead, as it does not yield any notable performance improvements compared to recursive character chunking.

Ultimately, while the evaluated chunking strategies demonstrated more comparable performance levels than initially expected, our findings identify recursive character chunking as the most robust solution for our application. Specifically, the configuration with a maximum chunk size of 512 tokens and zero overlap achieved high recall alongside promising precision values. Our overarching conclusion regarding the results of our evaluation is that tuning the parameters can be more crucial than the selection of the strategy itself.

Based on the results of our evaluation, we were able to identify multiple key findings regarding which metrics served useful for evaluating chunking configurations.

\paragraph{RQ2:} Completing our evaluation of suitable metrics, we identify the token-wise recall as the most important metric for measuring the efficacy of the chunking strategies. If the retrieval concludes with low recall, the generator will not have access to critical information needed to answer a query. This leads to inaccurate and ill informed answers from the system that could potentially lead to dubious medical recommendations. Simultaneously, token-wise precision remains a critical aspect of our analysis. Low precision indicates an increased ratio of distracting information included in the chunks, which can subsequently deteriorate the performance of the generator \autocite{including_distractions_rag,llm_context_distractions}. Additionally, low precision is detrimental to the accuracy of visual source attribution. If only a small amount of the highlighted text is relevant to the query, additional cognitive strain is placed on the user who has to manually review the content of the bloated bounding boxes. We find that the evaluation of different chunking strategies needs to take both metrics into account, albeit with a focus on maintaining a high token-wise recall.

We note that the inherent difficulty of the queries and the the varying amount of required context currently suppress the achieved evaluation scores. We hypothesize that with further retrieval optimization, both precision and recall could be drastically improved. Therefore, we find that $\text{precision}_\Omega$ proves to be an important indicator for the potential precision of the chunking configuration by establishing an upper bound for the retriever's precision. This metric essentially measures how accurately the created chunks align with the semantical structure of the document, while controlling for the performance of the retriever.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{images/iou_example}
  \caption[Visual example for the token-wise \gls{iou} of two different chunking strategies]{Visual example for the token-wise \gls{iou} of two different chunking strategies. For a given query and chunking strategy, the tokens from the blue area are retrieved, while the tokens from the yellow area are needed to answer the question. The set of correctly retrieved tokens is denoted by the red area. We see that despite the chunk from chunking strategy 2 providing the entirety of the needed information needed to answer a question, the \gls{iou} score is the same as strategy 1 due to the lower precision of strategy 2.}\label{fig:iou_example}
\end{figure}

On the other hand, our results indicate that the token-wise \gls{iou} is not a suitable metric for this evaluation, as it is heavily biased towards the achieved precision score. Fundamentally, the \gls{iou} does not differentiate between the two bounding boxes when determining their union. For the token-wise \gls{iou}, this means that retrieving an irrelevant token is penalized the same as not retrieving a relevant token. We highlight the issues of this metric through a visual example in \autoref{fig:iou_example}. As recall is the most critical aspect of our evaluation, valuing it the same as precision does not reflect the nature of the retrieval process. While the generator is able to tolerate some noise, missing important information that is needed to answer the query can not be made up for in subsequent stages.



% This means that retrieved irrelevant tokens far outnumber unretrieved relevant tokens. Increasing recall therefore does not change the value of the token-wise \gls{iou}



% While in object detection


% In object detection, \glspl{gtbb} and \glspl{dtbb} are similarly sized, which makes the \gls{iou} a viable metric. For retrieval in \gls{rag}, the ``prediction'' (retrieved context) is most often several magnitudes larger than the ground truth information. Even a drastic increase in recall therefore has an



% \begin{enumerate}
%   \item Increasing chunk size does not automatically lead to higher recall (Fixed size maxed at N=512 and Hierarchical stagnated across chunk sizes)
%   \item We observed that there was no clear winner for the best chunking strategy
%   \item However we did find that choosing the right parameters is crucial for the strategies
%   \item Recall is good because it means that we have more relevant information to construct our answer
%   \item Precision is good because the generator is less distracted by irrelevant information. Additionally, Precision is important for visual source attribution, as low precision also means that bounding boxes enclose more irrelevant information, making the user search through more text to find the relevant content.
%   \item Precision Omega measures how good the chunks align with the content of the retrieved text. Better alignment means that chunks generally fit the requested content well.
%   \item Overlap has a great benefit for strategies that provide chunks that are not aligned to the structure or semantics of the document. However when chunks are structured they introduce additional noise, leading to a decrease in precision
%   \item A higher threshold needs larger topical shifts to warrant a chunk splitting, which leads to fewer larger chunks, at lower levels a lower recall is
%   \item iou is not a viable metric as at the low precision levels it does not accurately measure the tradeoff relationship between precision and recall
% \end{enumerate}

% RQ 3:
% \begin{enumerate}
%   \item We improved upon the parsing process by adding additional post-processing steps to the method's outputs. Specifically, by inferring the hierarchical tree structure from the documents, we are able to model the relationships between the elements and enable novel chunking strategies that take advantage of the document structure. Our largest contribution comes in the form of our token-centric chunking methodology, which enables traceability and granular visual source attribution for any chunking strategy.
% \end{enumerate}

% The evaluation results are close to each other

% Based on $\text{Precision}_\Omega$, we see that strategies that follow more complex approaches produce chunks that are inherently better aligned with the underlying structure of the document. We see that these strategies include fewer unneeded tokens.

% For research question 3:
% We proposed a novel solution aimed at enabling traceability of the chunks tokens to its constituent ParsingResult nodes.

% \section{}


% \subsection{draftrq}

% We can answer RQ1 with the following:

% % PUBLAYNET
% We find that for \gls{dla} analysis PubLayNet incorporates many of the identified attributes of oncology guidelines. As the entirety of the dataset consists of medical journal literature, the application domain of the documents is very similar. As such, documents from PubLayNet incorporated the same text-centric layout in both double and single column variations. In addition, the documents contain large complicated tables and figures which is consistent with the oncology guidelines. PubLayNet only consists of vertical documents, missing the variations in page orientations of oncology guidelines. One major drawback of current \gls{dp} datasets is that they are usually solely comprised of single-page \gls{pdf} documents. This makes evaluations including multi-page structural elements, such as the tables in many oncology guidelines, impossible.

% We can therefore answer RQ2 with the following:

% % DLA METRIC
% The F1 score is a suitable metric for measuring the \gls{dla} capabilities of the \gls{dp} approaches. Due to its invariance towards confidence scores it enables the comparison of \glspl{vlm} and other hard predictors, which is a critical aspect in the evolving landscape of \gls{dp} implementations. In contrast to previous evaluations we are therefore able to measure the \gls{dla} performance of an entire \gls{dp} system, evaluating the quality of the final bounding box output including eventual post-processing steps. By evaluating the metric at multiple \gls{iou} values, we are both able to draw conclusions about the overall accuracy of the \gls{dla} process as well as the quality of the predicted bounding boxes. Additionally, reporting the metric individually for each category aids with identifying weak points of the \gls{dp} approach, especially for less common element types.

% % DP METRIC
% The selected metrics allow the analysis of the content extraction performance of \gls{dp} systems across multiple dimensions. Being able to analyze the table structure recognition separately using $TEDS-S$ proofs valuable for identifying which implementations are good at understanding tables, while controlling for the text recognition capabilities of the models. The normalized edit distance proofs useful as an analogy for the average amount of mistakes that the implementation made regarding the text understanding. Reading order hereby measures how well the implementations were able to adapt to the layout of the documents, understanding the structure of multi-column layouts.

% We can answer RQ3 with the following:

% IMPROVING THE SYSTEM



% These findings also support the principle of multi-stage \gls{vlm}-based approaches being able to perform better on individual tasks than single-stage \gls{vlm}

% There are multiple explanations for the zero-scores reported by Document AI and LlamaParse. LlamaParse does not have a figure or image type, which makes it impossible for the system to extract this category. As for list, the most likely explanation is that lists are often very similar to texts visually. So often the bullet point was interpreted as an extra character leading to Document AI classifying the output as text. This also explains the drop in performance for the list type across the other implementations.

% LlamaParse's bad performance can also be explained through its poor bounding boxes, sometimes even yielding invalid bounding boxes with negative coordinates. Many implementations struggled on the figure category as often figures that are placed next to each other were recognized as a single figure.\ \glspl{vlm} seem especially vulnerable to this effect. Granite Docling's performance suggests that its extremely compact 256M parameter architecture can not adapt well to the challenges of this dataset. 

%The change in hierarchy after increasing the iou value suggests that while the MinerU systems are generally better at predicting bounding boxes with an acceptable accuracy, the correct bounding boxes produced by Docling are more accurate to the ground truth. The sharp drop in title when increasing the iou value compared to other categories can be explained by the smaller average area of the bounding boxes for titles, such that there is less total error possible as the iou value increases. For large elements such as tables, even at an iou of 95\% there is still some space for errors. MinerU 2.5 VLM's performance drop suggests that, as an allrounder model that handles multiple tasks, the quality of its output for \gls{dla} may be subpar compared to the pipeline-based approach.