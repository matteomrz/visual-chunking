% !Tex root = ../main.tex

\chapter{Foundations}\label{chapter:foundations}

\section{Oncology guideline documents}\label{section:oncology_guidelines}
\Glspl{cpg} help improve patient care by giving recommendations on the optimal treatment and prevention of various diseases \autocite{cpg_good_bad_ugly,oncology_review}. They are developed by groups of independent multi-disciplinary experts and are based on a robust systematic review of available treatment options and knowledge gained from clinical experience \autocite{cpg_trust,cpg_good_bad_ugly,oncology_review}. Instead of dictating a single definitive treatment option, \glspl{cpg} instead focus on aiding the decision making process, promoting treatment options with proven benefits and discouraging ineffective or harmful treatments \autocite{cpg_trust,cpg_good_bad_ugly}. As such, they aim to improve the quality of the provided health care by encouraging the translation of research into medical practice \autocite{oncology_review}. Oncology guidelines are a subgroup of this document type, focusing on the treatment and rehabilitation options for various types of cancers \autocite{oncology_review}.

In order to further improve the quality and standardization of oncology guidelines \autocite{oncology_review,guidelines_length}, and therefore cancer care, several prominent organizations have emerged, which endorse and publish selected oncology guidelines. Prominent examples include the \gls{nccn} \autocite{nccn_about_guidelines}, the \gls{esmo} \autocite{esmo}, and, for oncology guidelines in the German language, the \glspl{awmf} \autocite{awmf}. Over the last decades oncology has seen many advances in the research and treatment outcomes of many forms of cancers \autocite{guidelines_length,advancements_oncology}. Following these findings and advancements, the number of available treatment options has increased drastically \autocite{guidelines_length}. This increase in available treatments is ultimately reflected in the increasing complexity of oncology guidelines.\ \textcite{guidelines_length} found that, between 1996 and 2019, the mean page count of guidelines published by the \gls{nccn} has increased from 26 to 198 pages, with the number of referenced citations per guideline also increasing from an average of 30 to 111.

In order to identify common characteristics between the layout, typography and page design of different oncology guideline documents, we perform a qualitative analysis on a selection of German and English guideline documents from multiple publishing organizations. Despite significant variability between guidelines from different publishers, several shared characteristics can be observed:

\paragraph{Data format:} The primary data format for digital distribution of oncology guidelines is the \gls{pdf}.\ \gls{pdf} is a data format designed to enable the reliable distribution and viewing of electronic documents independent of the viewing or creating environment \autocite{pdf_iso}. Particularly, these documents are born-digital \gls{pdf} files, created through digital processes, instead of scanning analog documents.

\paragraph{Page geometry:} All observed documents are provided in the standard A4 format, thereby sharing common page dimensions. While the majority of oncology guidelines are provided in a vertical orientation, both horizontal and mixed page orientations are possible. Additionally, there exist some cases where two neighboring vertical pages are contained in a single horizontal page.

\paragraph{Content and layout:} The formatting and content of the guideline documents is heavily dependent on their target audience. ``Standard'' guideline documents, addressing medical professionals, resemble typical scientific documents. They are mostly provided in a single or double-column layout, and, due to their focus on aggregating the results of previous studies, are predominantly text-heavy. Additionally, they often contain complex tables which may span multiple pages, primarily to compare different treatment options against each other. While less frequent, figures, mathematical formulas and images are also occasionally included. As \glspl{cpg} are often too complicated for patients to understand, some publishers provide ``patient guidelines'' alongside their \glspl{cpg}. These documents translate the recommendations from the \gls{cpg} into a language that is understood by the general population, while leaving out scientific details that are less relevant to the patient. Compared to the \gls{cpg} these documents usually incorporate more figures and visual elements while offering more variability in their typography and page designs.

\paragraph{Document quality:} Depending on the \gls{cpg}'s age and publishing organization, the formatting of the document may contain significant structural errors. Observed formatting issues include overlapping text, empty pages between content, tables extending into the page margins, and invisible text on document pages.

\section{Natural Language Processing Fundamentals}
According to \textcite{nlp_advances}, \gls{nlp} \textquote[p.1]{employs computational techniques for the purpose of learning, understanding, and producing human language content}. The introduction of the transformer architecture by \textcite{transformer} and the subsequent development of \glspl{llm} has revolutionized the field in recent years \autocite{llm_in_nlp}. In order to understand how \glspl{llm} process and perceive information, it is necessary to examine various fundamental concepts.

\subsection{Tokenization}
Tokenization refers to the segmentation of text into sub-word units called tokens \autocite{fast_tokenization}. Tokens are the fundamental text representation for most \gls{nlp} tasks. With a granularity located between characters and words, tokens can retain linguistic meaning while also being able to represent arbitrary text with a relatively concise vocabulary \autocite{fast_tokenization}. Using tokenization any given text can essentially be represented as a list of integers, with each integer being the identifier to a specific token in the tokenizer's dictionary \autocite{token_history}. During training, the tokenizer creates its dictionary by finding character pairings that occur with the highest frequency in the training data \autocite{token_history}. Additionally, with the multitude of different techniques for modern sub-word tokenization \autocite{wordpiece,sentencepiece,unigram}, the same input text can lead to drastically different outputs depending on the specific tokenizer and training data. Therefore, tokenizers always need to match the \gls{nlp} models they are used with.

\subsection{Sentence Embeddings}
Sentence embeddings encode the semantical meaning of sentences into vectors of fixed-dimensionality \autocite{speech_and_language}. Every modern \gls{nlp} algorithm uses embeddings as the representation of the meaning of texts \autocite{speech_and_language}. Using this technique, the meaning of the text is transformed into a machine-understandable format, with the embedding vectors of closely related sentences being closer to each other in the vector space.

\subsection{Cosine Similarity}
The cosine similarity determines the similarity between two sentences by calculating the cosine of the angle between the embedding vectors $v$ and $w$. Cosine similarity builds on top of the dot product metric (\autoref{eq:dot_product}). The dot product tends to be high when $v$ and $w$ have large values in the same dimensions, therefore measuring their similarity \autocite{speech_and_language}.

\begin{equation}
  \text{dot product}(v,w)=v\cdot w=\sum_{i=1}^{N}v_i w_i=v_1w_1+v_2w_2+\dotsc+v_N w_N
  \label{eq:dot_product}
\end{equation}

However, the dot product is not invariant to the length of the vector, defined in \autoref{eq:length_v}, producing higher values for vectors of greater length \autocite{speech_and_language}. This leads to skewed similarity values if vectors are not normalized prior.

\begin{equation}
  |v|=  \sqrt{\sum_{i=1}^{N}v_i^2}
  \label{eq:length_v}
\end{equation}

The cosine similarity is calculated as the normalized dot product, as defined in \autoref{eq:cosine_similarity}. As such, it is a measurement of the similarity between two vectors that is invariant to their length \autocite{speech_and_language}. The metric is identical to the cosine of the angle between the vectors $v$ and $w$, as seen in \autoref{eq:angle_cosine}. The cosine similarity is by far the most commonly used similarity metric.

\begin{equation}
  \text{cosine}(v,w)=\frac{v\cdot w}{|v||w|}=\frac{\sum_{i=1}^{N}v_i w_i}{\sqrt{\sum_{i=1}^{N}v_i^2}\sqrt{\sum_{i=1}^{N}w_i^2}}
  \label{eq:cosine_similarity}
\end{equation}

\begin{equation}
  \begin{split}
    a\cdot b                & =|a||b|\cos\theta \\
    \frac{a\cdot b}{|a||b|} & =\cos\theta
    \label{eq:angle_cosine}
  \end{split}
\end{equation}

\section{Vision-Language Models}
\Glspl{llm} are inherently confined to processing exclusively text-based data. This limitation restricts their applicability in complex, real-world scenarios, where understanding and combining data from multiple modalities is crucial \autocite{vlm_frontier,qwen25vl}.\ \Glspl{vlm} are a class of models which respond to these limitations by combining visual and textual processing capabilities into a single architecture \autocite{vlm_frontier}. These models find applications involving both the comprehension and generation of multi-modal content, such as image captioning, and visual question answering \autocite{vlm_frontier}.

% The typical architecture of \glspl{vlm} is depicted in \todo{figure}. The architecture consists of a text encoder, an image encoder, an image-text

% According to \textcite{vlm_frontier}, \glspl{vlm} can be categorized into three classifications.

% \textbf{Vision-Language Understanding:} \Gls{vlu} includes models which are designed to comprehend and interpret visual and textual data and their relationships. Common applications for models of this classification are image classification and object detection \autocite{vlm_frontier}. Notable examples of \gls{vlu} models include CLIP \autocite{clip} and VLMo \autocite{vlmo}.

% \textbf{Text Generation with Multi-modal Input:} Models of this classification excel in creating textual content by incorporating information from both visual and textual sources. Popular examples of this modality are the GPT family of models as well as the open-source Qwen

% \textbf{Multi-modal Output with Multi-modal Input:} These models further enhance the capabilities from previous groups by being able to generate both textual and visual content.

% \Glspl{vlm} extend the capabilities of traditional \glspl{llm}, adding visual understanding to their natural language processing abilities \autocite{qwen25vl}.

% \todo{Maybe need to add explanations for transformers?}\\

\subsection{Bounding Boxes}
Bounding boxes represent the most fundamental method for annotating the position of an object within an image. A bounding box is the smallest rectangle that fully encloses the shape of the object \autocite{shape_analysis}. These boxes are defined within the image's coordinate system, with its origin typically positioned at the top-left corner of the image \autocite{dive_into_dl}. The x-axis extends horizontally from this point, while the y-axis extends vertically. Coordinates can either be expressed in absolute pixel units or as normalized fractional values relative to the dimensions of the image. For this study, we focus exclusively on horizontal bounding boxes, which are aligned to the horizontal axis, also known as Feret Boxes \autocite{shape_analysis}. There are multiple formats for representing bounding boxes, with the \gls{ltrb} notation, which denotes the coordinates of the top-left and bottom-right corners of the bounding box, being a prominent option \autocite{dive_into_dl}.

\section{Retrieval-Augmented Generation}
Although \glspl{llm} have extensive general domain knowledge due to their enormous corpora of training data, compiled from various open-domain sources \autocite{impact_dataset_rag_multi_hop}, they struggle with tasks that require domain-specific knowledge which they did not encounter during training \autocite{rag_survey}. This can lead to ``hallucinations'' and inaccuracies, as the model tries to synthesize a matching answer based of its domain-wise irrelevant training data \autocite{rag_survey,chunk_size_effect_on_rag}.\ \gls{rag} addresses this limitation, extending the usage of \glspl{llm} to applications requiring extensive knowledge in a specific domain \autocite{rag}. This is achieved by retrieving information from an external knowledge source comprised of application-relevant text passages, supplying additional context to the \gls{llm} during answer generation \autocite{rag,rag_survey}.

\subsection{Architecture of RAG Systems}
While there are many advanced and extended versions of \gls{rag} systems, for this study we will focus on the standard Naive \gls{rag} architecture, as depicted in \autoref{fig:naive-rag} \autocite{rag_survey}. Naive \gls{rag} is based on the original \gls{rag} architecture proposed by \textcite{rag}. Naive \gls{rag} systems consist of two modules:

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{images/rag_diagram}
  \caption[Naive \gls{rag}]{Architecture of the Naive \gls{rag} system.}\label{fig:naive-rag}
\end{figure}

\paragraph{Retriever:} The retriever module consists of a query encoder and an external knowledge base \autocite{rag}. It is responsible for retrieving relevant context from the knowledge base, based on the user's query \autocite{rag_survey}. The module is based on the bi-encoder architecture, with the query encoder $q$ and document encoder $d$ encoding texts into a shared embedding space \autocite{rag, dual_encoders}. The knowledge base is a vector database consisting of application-specific text passages $z$. Each passage is stored in the database as a vector embedding $d(z)$, encoded through the document encoder $d$ \autocite{rag}. To identify the relevant passages for a query $x$, $x$ is first transformed into a vector embedding $q(x)$ using the retriever's query encoder \autocite{rag_survey}. Based on the similarity scores between the query embedding and the stored chunk embeddings, the top-$k$ documents $z$ with the highest similarity scores, are then retrieved from the database \autocite{rag,rag_survey}.

\paragraph{Generator:} The generator module is responsible for synthesizing the final answer based on the user's query and the passages retrieved by the retriever \autocite{rag}. Firstly, the original query $x$ and the retrieved passages $z$ are combined into a single input query \autocite{rag_survey}. The \gls{llm} is then tasked with generating the final answer $y$, conditioned on this combined input \autocite{rag_survey,rag}.

\subsection{Indexing}
In order to apply the \gls{rag} paradigm to knowledge-intensive tasks in a specific domain, the external knowledge base needs to be created from relevant data sources. This process is called Indexing \autocite{rag_survey}. Indexing begins with the preparation of the data sources into short text passages \autocite{rag_survey}. For the purpose of this study we will refer to this process as document segmentation. Document segmentation includes both \gls{dp}, the conversion of unstructured documents, such as \glspl{pdf} and images, into structured data \autocite{parsingunveiled}, as well as chunking, the splitting of this data into smaller text passages called chunks \autocite{rag_survey}. Chunking is a necessary step for \gls{rag} systems, as both \glspl{llm} and encoders are limited in the number of tokens that fit into their context window \autocite{rag_survey}. Furthermore, indexing includes the encoding of these chunks into vector embeddings. Both the embeddings and the original chunks are then stored as key-value pairs in a vector database, allowing fast and frequent searches during retrieval \autocite{rag_survey}.

The quality of the index construction has a crucial effect on the resulting \gls{rag} system \autocite{rag_survey}. It determines both the likelihood of retrieving relevant context as well as the quality of the generated answer. Especially chunking, which is often overlooked and seen as solely a technical requirement, has been found to be crucial for enhancing the quality of the knowledge base \autocite{rag_survey}.

\subsection{Source Attribution}
Source attribution is a mechanism that provides transparency and traceability to the output of the \gls{rag} system by linking the generated text to their source documents \autocite{visa,llm_citations}. This allows the user to verify the \glspl{llm} claims by examining the provided sources \autocite{llm_citations}. Source attribution can be performed at different granularity levels. Document level source attribution provides citations to the entire documents that the retrieved passages belong to \autocite{visa,llm_citations}. While this approach enables the necessary verifiability, it introduces additional strain to the user, who has to find the relevant passages in the document \autocite{visa}. This effect is amplified for longer documents, such as \glspl{cpg}. In order to mitigate this issue, recent research has suggested the concept of visual source attribution \autocite{visa}. Visual source attribution revolves around visual confirmation for the exact location of the retrieved information \autocite{visa}. This is achieved by highlighting the exact region of the retrieved text inside of the document \autocite{visa}. The position of the retrieved passage is therefore immediately visible to the user, making source attribution easy and seamless.

\section{Document Parsing}
Also known as document content extraction, \gls{dp} aims to convert unstructured and semi-structured documents into structured, machine readable data formats \autocite{parsingunveiled,omnidocbench}. During this process elements, such as headings, tables, and figures, are extracted from the document while preserving their structural relationships.\ \gls{dp} is crucial for many document-related tasks, providing access to previously unavailable information sources. Especially for \glspl{llm}, where leveraging additional training data is crucial for enhancing the model's factual accuracy and knowledge grounding, \gls{dp} plays an important rule \autocite{omnidocbench,mineru}. With the emergence of the \gls{rag} paradigm, \gls{dp} has also been critical in the creation of the knowledge database, as important information is often stored inside file formats which can not directly be processed by machines \autocite{docling}. While \gls{dp} is used for converting a range of document formats into machine-readable content, we will focus solely on the parsing of \gls{pdf} documents for the purposes of this thesis, as this is the data type that the oncology guidelines are stored as.

Converting \gls{pdf} documents is particularly challenging due to their variable formatting, lack of standardization and focus on visual characteristics \autocite{docling}. The format not only includes born-digital files but also includes photographed and scanned documents. Therefore, \gls{dp} systems need to be able to adapt to a wide range of different layouts, image qualities and document types, such as academic papers, invoices, or presentation slides \autocite{omnidocbench,intelligent_doc_parsing}. While there are many tools and implementations available for \gls{dp} \autocite{docling,mineru,mineru_vlm,unstructuredio}, most of them can be categorized into either modular pipeline systems or end-to-end VLM models.

\subsection{Modular Pipeline Systems}
Modular pipeline systems employ various different modules in a sequential order to perform \gls{dp}. This modular design enables the targeted optimization of individual components and flexible integration of new modules and techniques \autocite{monkeyocr}. Additionally, by making use of lightweight models and integrating parallelization, pipeline systems can reach efficient parsing speeds \autocite{omnidocbench}. While different formations are possible, most implementations consist of three different stages \autocite{parsingunveiled}.

\paragraph{\Glspl{dla}:} According to \textcite{parsingunveiled}, \gls{dla} refers to the identification of the structural elements of a document, such as paragraphs, section headings, tables, figures, and mathematical equations, as well as their respective bounding boxes \autocite{parsingunveiled,docling_heron}. There are two types of methods for performing \gls{dla}. Uni-modal methods focus purely on visual features of the document in order to identify structural elements \autocite{parsingunveiled,pp_doclayout}. Notably, \gls{cnn}- and transformer-based methods adapt models initially designed for object detection tasks, such as the YOLO \autocite{yolo} and DETR \autocite{detr} families of models, to accurately identify structural elements in document images \autocite{parsingunveiled,docling_heron}. Hereby, transformer-based methods excel at capturing global relationships between structural elements at the cost of computational intensity and expensive pre-training \autocite{parsingunveiled}.
The second type of \gls{dla} methods are multi-modal methods. In addition to the visual representations, multi-modal methods also make use of the content and position of the pages' textual elements, performing \gls{dla} using a \gls{vlm} \autocite{pp_doclayout,layoutlm_v3}. This approach allows more granular classifications and the analysis of highly complex layouts \autocite{parsingunveiled,pp_doclayout}.

\paragraph{Content Extraction:} To extract the content of the identified structural elements different recognizers are applied to the element regions based on their classifications \autocite{parsingunveiled,mineru,docling}. For textual elements, such as paragraphs or section headings, the textual content is identified using \gls{ocr}.\ \gls{ocr} engines use techniques from computer vision in order to identify and extract text from images \autocite{parsingunveiled,ocr_survey}. Popular \gls{ocr} engines include EasyOCR \autocite{easyocr} and the Tesseract OCR engine \autocite{tesseract}. In addition to extracting content using \gls{ocr}, \gls{dp} implementations often provide specific recognizers for additional element types \autocite{parsingunveiled,mineru}. Most commonly this includes a specific model for table structure recognition, referring to the extraction of table content into structured file formats, such as \gls{html}, \gls{xml} or Markdown \autocite{parsingunveiled,docling,mineru,unstructured_open_source}. Other options for class-specific recognizers include mathematical formula recognition and chart recognition \autocite{mineru,mineru_vlm,parsingunveiled}.

\paragraph{Relation Integration:} During relation integration the identified elements are combined into the final output format. During this stage, rule-based methods and specialized \gls{ai} models may be employed, for example to filter out duplicate or unwanted elements or correct the reading order of the document \autocite{parsingunveiled,mineru,docling}. Depending on the chosen output format, this process might lead to the loss of information, such as the loss of bounding box information for an output in Markdown format \autocite{docling_toolkit}.
\\ \\
Systems that follow the modular pipeline approach also have some inherent drawbacks. Mainly, due to handling the parsing of each structural element independently of each other, pipeline systems fail to capture information about the global context of the document, leading to semantic loss \autocite{intelligent_doc_parsing}. Additionally, because of the sequential nature of the pipeline approach, errors from different stages propagate through the pipeline \autocite{intelligent_doc_parsing,mineru_vlm}.

\subsection{End-to-End VLM models}
Due to recent advancements in \gls{vlm} architectures, end-to-end \gls{vlm} models have emerged as a promising alternative to traditional pipeline-based approaches. Research, such as the \gls{got}, have demonstrated the ability of \glspl{vlm} to perform high accuracy \gls{ocr} while being able to extract the content of tables, charts, or mathematical formulas using a singular model \autocite{general_ocr_theory}. Contrary to pipeline-based methods, \gls{vlm}-based approaches are able to generate structured outputs directly from the input document, addressing the error propagation problem of modular pipelines \autocite{monkeyocr}. Additionally, these models demonstrate advantages in understanding the structure and hierarchy of complex documents \autocite{parsingunveiled}.\ \gls{vlm}-based approaches can be divided into two further subcategories:

\paragraph{General-Purpose \glspl{vlm}:} General purpose \glspl{vlm} are not trained exclusively for document-centric tasks, but are still able to show promising results for \gls{dp}, due to their large parameter count and extensive training data \autocite{mineru_vlm,qwen25vl}. However, these models are often either proprietary or require extensive computational resources \autocite{mineru_vlm}. Additionally, they often struggle with documents that follow more complex layouts or contain densely packed text blocks \autocite{mineru_vlm}.

\paragraph{Domain-Specific \glspl{vlm}:}
Domain-specific \glspl{vlm} are trained and optimized specifically for \gls{dp} \autocite{mineru_vlm,parsingunveiled,intelligent_doc_parsing}. In recent years, there has been promising developments towards domain-specific \glspl{vlm} that encapsulate \gls{dla}, content extraction and relation integration into a single model \autocite{dotsocr}. These models are able to achieve state-of-the-art performance on \gls{dp} benchmarks, while being a fraction of the size of general-purpose \glspl{vlm} \autocite{dotsocr,mineru_vlm}. As \glspl{vlm} are not bound to the stages of traditional pipeline systems, there has also been additional research regarding models that are optimized for the direct generation of content-only outputs, most notably Markdown \autocite{intelligent_doc_parsing}. However, this approach inherently leads to the loss of information, such as positional information for the extracted elements, which is not included in the Markdown format, making this class of models unsuitable for the purposes of this research \autocite{dotsocr,intelligent_doc_parsing}.

Recently, there has also been research towards multi-stage \gls{vlm}-based approaches \autocite{mineru_vlm,monkeyocr}. These models use one or more \glspl{vlm} in multiple stages, aiming to combine the computational efficiency of pipeline approaches with the improved accuracy and structure understanding of \gls{vlm}-based methods \autocite{mineru_vlm}. However, especially when multiple \glspl{vlm} are in use, these approaches come with a further increase in complexity and computational requirements and may show decreased performance in tasks such as reading order inference compared to single-stage \gls{vlm}-based approaches \autocite{mineru_vlm,dotsocr}. Current challenges regarding the development of \gls{vlm}-based approaches are the risks of ``hallucinations'', especially on longer documents \autocite{mineru_vlm,docling_toolkit}, as well as their high computational requirements compared to modular pipeline systems \autocite{docling_toolkit}.

\section{Chunking}
Chunking refers to the splitting of documents into small atomic units of information called chunks \autocite{chroma_eval,rag_survey}. While the term is directly linked to the recent emergence of the \gls{rag} paradigm, the underlying task of text division is fundamentally aligned to the established concept of passages in passage-based document retrieval \autocite{passage_based_retrieval,passage_94}.

Despite the rapid adoption of \gls{rag}, the chunking process lacks a robust scientific taxonomy. Much of the terminology associated with modern chunking strategies originates from non-scientific sources, such as technical blogs, software documentation, and community tutorials. We find that the established taxonomy of passage-based document retrieval aligns with the types of modern chunking strategies. To ensure scientific stability, we therefore adopt the terminology proposed by \textcite{passage_94}. Specifically, \textcite{passage_94} categorizes passages into three distinct types: window passages, semantic passages, and discourse passages.

\subsection{Window Passages}
Window passages are determined by splitting the content of the document into parts of a fixed length. While in passage-based retrieval, length typically referred to the number of words in a passage \autocite{passage_94}, with the advent of chunking the focus shifted towards measuring the number of tokens \autocite{chroma_eval}. Modern chunking strategies have further extended this method through sliding-window approaches, which introduce a fixed overlap between neighboring chunks to preserve contextual continuity \autocite{llamaindex_chunking}. These strategies provide a simple and computationally efficient way to perform chunking \autocite{semantic_chunking}. However, they disregard the content of the document, which may result in chunk borders appearing inside a single word or sentence \autocite{chunking_comparison}.

\subsection{Semantic Passages}
Semantic passages aim to enhance retrieval quality by aligning passage borders to identified subtopics of the document \autocite{chunking_comparison}. However, they introduce significant additional computational complexity and may vary drastically in length \autocite{semantic_chunking}. Strategies from this category stem from the field of text segmentation, referring to \textquote[p.1]{the task of dividing text into segments, such that each segment is topically coherent, and cutoff points indicate a change in topic} \autocite{text_segmentation}. In recent years there have also been novel strategies proposed for this task that leverage \glspl{llm} to determine semantically independent chunks \autocite{lumberchunker}.

\subsection{Discourse Passages}
Discourse passages are defined by the inherent structure of the document, such as sections, sentences, and paragraphs. Typically, these strategies recursively divide the document with increasing granularity until resulting chunks satisfy a specified maximum length constraint \autocite{langchain_splitting_recursively}. While the documents in passage-based document retrieval are simple unstructured text streams \autocite{passage_94}, modern chunking techniques often process data in structured formats such as \gls{json} or \gls{xml} \autocite{docling_toolkit}, especially when combined with \gls{dp}. Recently, specialized strategies have emerged that leverage additional metadata from these formats, such as hierarchical relationships between elements, to produce chunks that follow the structure of the document more closely \autocite{docling_toolkit}.

\subsection{Metadata Attachments}
In addition to their textual content, chunks can be enriched with metadata information \autocite{rag_survey}. This metadata can include information about the original document, such as its author, title, or publishing date. This enables the filtering of retrievable data based on document attributes, such as limiting the retrieval to documents published in a specific time frame \autocite{rag_survey}. Metadata attachments are also critical for providing source attribution. While document information enables traceability at the document level, additional metadata such as the page number and bounding box of the chunk achieves more granular grounding.
