% !Tex root = ../main.tex

\chapter{Foundations}\label{chapter:foundations}

\section{Oncology guideline documents}
Qualitative analysis of the properties of oncology guideline documents.

\section{Retrieval-Augmented Generation}
While \glspl{llm} have extensive general domain knowledge due to their enormous corpora of training data, compiled from various open-domain sources \autocite{impact_dataset_rag_multi_hop}, they struggle with tasks that require domain-specific knowledge which they did not encounter during training \autocite{rag_survey}. This can lead to `hallucinations' and inaccuracies, as the model tries to synthesize a matching answer based of it's domain-wise irrelevant training data \autocite{rag_survey,chunk_size_effect_on_rag}.\ \gls{rag} addresses this limitation, extending the usage of \glspl{llm} to applications requiring extensive knowledge in a specific domain \autocite{rag}. This is achieved by introducing an external knowledge source comprised of application-relevant text passages, stored as embeddings in a high-dimensionality vector database \autocite{rag}.

When the user sends a new query to the system, the retriever model first retrieves additional context from the vector database by determining the most relevant text passages based on their semantic similarity to the query \autocite{rag}. These passages are then prepended to the user's original input and forwarded to the generation model. By providing this additional context, the \gls{llm} can synthesize answers grounded in the retrieved passages instead of relying solely on it's training data \autocite{rag,rag_survey}.

Content:
\begin{itemize}
  \item Definition
  \item Components
  \item Use cases
\end{itemize}
\todo{Other usages of chunking than RAG}

\subsection{Tokenization}
Tokenization refers to the segmentation of text into subword units called tokens \autocite{fast_tokenization}. Tokens are the fundamental text representation for most \gls{nlp} tasks. With a granularity located between characters and words, tokens can retain linguistic meaning while also being able to represent arbitrary text with a relatively concise vocabulary \autocite{fast_tokenization}. Using tokenization any given text can essentially be represented as a list of integers, with each integer being the identifier to a specific token in the tokenizer's dictionary \autocite{token_history}. During training, the tokenizer creates it's dictionary by finding character pairings that occur with the highest frequency in the training data \autocite{token_history}. Additionally, with the multitude of different techniques for modern subword tokenization \autocite{wordpiece,sentencepiece,unigram}, the same input text can lead to drastically different outputs depending on the specific tokenizer and training data. Therefore, tokenizers always need to match the \gls{nlp} models they are used with.

\subsection{Text Embeddings}
Content:
\begin{itemize}
  \item Definition
  \item Types of embeddings
\end{itemize}

\subsection{Semantic Similarity}
Content:
\begin{itemize}
  \item Definition
  \item Role in RAG
\end{itemize}

\subsection{Source Attribution}


\subsection{Bounding Boxes}
\begin{itemize}
  \item Definition
  \item Formats of Bounding boxes
  \item What do we mean when we say bounding boxes in the thesis? (normalized, ltrb)
\end{itemize}

\subsection{Intersection over Union}
According to the definition from \textcite{object_detection}, the \gls{iou} between two bounding boxes $BB_{a}$ and $BB_{b}$ is defined as described in \autoref{eq:iou}. The \gls{iou} can take on any value between 0 and 1, where a value of 0 means that there is no overlap between the two bounding boxes, and a value of 1 means that the two bounding boxes are identical. In the context of object detection, \gls{iou} is commonly used to evaluate the accuracy of predicted bounding boxes against ground truth bounding boxes \autocite{object_detection}.

\begin{equation}
  IoU(BB_{a}, BB_{b}) = \frac{\text{Area of intersection of } BB_{a} \text{ and } BB_{b}}{\text{Area of union of } BB_{a} \text{ and } BB_{b}}
  \label{eq:iou}
\end{equation}

\section{Vision-Language Models}
\todo{Maybe need to add explanations for transformers?}\\
Content:
\begin{itemize}
  \item Definition
  \item Role in RAG
\end{itemize}

\section{Document Parsing}
Also known as document content extraction, \gls{dp} aims to convert unstructured and semi-structured documents into structured, machine readable data formats \autocite{parsingunveiled,omnidocbench}. During this process elements such as headings, tables, and figures are extracted from the document while preserving their structural relationships.\ \gls{dp} is crucial for many document-related tasks, providing access to previously unavailable information sources. Especially for \glspl{llm}, where leveraging additional training data is crucial for enhancing the model's factual accuracy and knowledge grounding, \gls{dp} plays an important rule \autocite{omnidocbench,mineru}. With the emergence of the \gls{rag} pardigm, \gls{dp} has also been critical in the creation of the knowledge database, as important information is often stored inside file formats which can not directly be processed by machines \autocite{docling}. While \gls{dp} is used for converting a range of document formats into machine-readable content, we will focus solely on the parsing of \gls{pdf} documents for the purposes of this thesis, as this is the datatype that the oncology guidelines are stored as.

Converting \gls{pdf} documents is particularly challenging due to their variable formatting, lack of standardization and focus on visual characteristics \autocite{docling}. The format not only includes digitally-born files but also includes photographed and scanned documents. Therefore \gls{dp} systems need to be able to adapt to a wide range of different layouts, image qualities and document types, such as academic papers, invoices or presentation slides \autocite{omnidocbench,intelligent_doc_parsing}. While there are many tools and implementations available for \gls{dp}\autocite{docling,mineru,mineru_vlm,unstructuredio}, most of them can be categorized into either modular pipeline systems or end-to-end VLM models.

\subsection{Modular Pipeline Systems}
Modular pipeline systems employ various different modules in a sequential order to perform \gls{dp}. This modular design enables the targeted optimization of individual components and flexibility integration of new modules and techniques \autocite{monkeyocr}. Additionally, by making use of lightweight models and integrating parallelization, pipeline systems can reach efficient parsing speeds \autocite{omnidocbench}. While different formations are possible, most implementations consist of three different stages \autocite{parsingunveiled}.

\textbf{\Glspl{dla}:} According to \textcite{parsingunveiled}, \gls{dla} refers to the identification of the structural elements of a document, such as paragraphs, section headers, tables, figures, and mathematical equations, as well as their respective bounding boxes \autocite{parsingunveiled,docling_heron}. There are two types of methods for performing \gls{dla}. Unimodal methods focus purely on visual features of the document in order to identify structural elements \autocite{parsingunveiled,pp_doclayout}. Notably, \gls{cnn}- and transformer-based methods adapt models initially designed for object detection tasks, such as the YOLO \autocite{yolo} and DETR \autocite{detr} families of models, to accurately identify structural elements in document images \autocite{parsingunveiled,docling_heron}. Hereby, transformer-based methods excell at capturing global relationships between structural elements at the cost of computational intensivity and expensive pretraining \autocite{parsingunveiled}.
The second type of \gls{dla} methods are multi-modal methods. Additionally to the visual representations, multi-modal methods also rely on the content and position of the pages' textual content, performing \gls{dla} using a \gls{vlm} \autocite{pp_doclayout,layoutlm_v3}. This approach allows more granular classifications and the analysis of highly complex layouts \autocite{parsingunveiled,pp_doclayout}.

\textbf{Content Extraction:} To extract the content of the identified structural elements different recognizers are applied to the element's region based on it's classification \autocite{parsingunveiled,mineru,docling}. For textual elements, such as paragraphs or section headings, the textual content is identified using \gls{ocr}.\ \gls{ocr} engines use techniques from computer vision in order to identify and extract text from images \autocite{parsingunveiled,ocr_survey}. Popular \gls{ocr} engines include EasyOCR \autocite{easyocr} and the Tesseract OCR engine \autocite{tesseract}. Additionally to extracting content using \gls{ocr}, \gls{dp} implementation often provide specific recognizers for additional element types \autocite{parsingunveiled,mineru}. Most commonly this includes a specific model for table structure recognition, referring to the extraction of table content as a structured file format, such as HTML, XML or Markdown \autocite{parsingunveiled,docling,mineru,unstructured_open_source}. Other options for class-specific recognizers include mathematical formula recognition and chart recognition \autocite{mineru,mineru_vlm,parsingunveiled}.

\textbf{Relation Integration:} During relation integration the identified elements are combined into the final output format. During this stage, rule-based methods and specialized \gls{ai} models may be employed, for example for filter out duplicate or unwanted elements or correcting the reading order of the document \autocite{parsingunveiled,mineru,docling}. Depending on the chosen output format, this process might lead to the loss of information, such as the loss of bounding box information for an output in Markdown format \autocite{docling_toolkit}.

Systems implementing the modular pipeline approach also have some inherent drawbacks. Mainly, due to handling the parsing of each structural element independly of each other, pipeline systems fail to capture information about the global context of the document, leading to semantic loss \autocite{intelligent_doc_parsing}. Additionally, because of the sequential nature of the pipeline approach, errors from different stages propagate through the pipeline \autocite{intelligent_doc_parsing,mineru_vlm}.

\subsection{End-to-End VLM models}
\todo{Maybe add something about History?}
\begin{itemize}
  \item Definition
  \item Difference between Specialized and general vlms
  \item Examples
  \item Problems
\end{itemize}

\section{Document Chunking}

Content:
\begin{itemize}
  \item Definition
  \item Role in RAG systems
  \item Different Types: Rule based, File dependent
\end{itemize}
