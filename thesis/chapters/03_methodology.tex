% !Tex root = ../main.tex

\chapter{Methodology}\label{chapter:methodology}

\section{Pipeline Overview}
In order to compare different \gls{dp} implementations and chunking strategies against each other, we developed a modular document segmentation pipeline. The pipeline's architecture, illustrated in \autoref{fig:pipeline-overview}, follows a two-stage process. Firstly, raw \gls{pdf} documents are transformed into a structured data format. This data is then partitioned into metadata-enriched chunks. The core principle of the pipeline's design lies in its modularity, allowing the seamless interchange of both the used \gls{dp} implementation and the chunking strategy, while maintaining a unified interface for both modules.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{images/pipeline_diagram}
  \caption[Document Segmentation Pipeline]{Architecture of the document segmentation pipeline.}\label{fig:pipeline-overview}
\end{figure}

\paragraph{Parsing module:} The parsing module is the first step of the pipeline. It integrates eight different \gls{dp} implementations into a unified interface, normalizing their output formats into a standardized data format. Firstly, document elements and their structural information are extracted from the document using the underlying \gls{dp} implementation. The normalized output then undergoes further post-processing steps, such as the filtering of unwanted element types. Finally, the result of the parsing operation is persisted to the file system as both a lossless \gls{json} serialization and a lossy Markdown serialization for further processing and evaluating.

\paragraph{Chunking module:} The chunking module is responsible for the splitting of the structured data into smaller chunks using one of multiple available chunking strategies. We adapt four established strategies to operate on the data format provided by the parsing module. Additionally, we propose a novel approach to the chunking paradigm, enabling traceability of the chunk's content on the token level. This allows for the determination of more accurate chunk bounding boxes, enabling high granularity, visual source attribution for downstream \gls{rag} applications.

\section{Data Representations}
A significant challenge for the evaluation and comparison of different \gls{dp} implementations is the lack of standardization. As of the time of writing, every \gls{dp} implementation defines their own data types, making direct comparisons very complex. In order to consolidate multiple different \gls{dp} implementations into our modular pipeline and evaluate them against each other, we define our own universal data types to be used for the document segmentation process. Before further processing, the outputs of each \gls{dp} implementation are first transformed into these data types.

\subsection{ParsingResultType}
A central issue arising from the fragmented methodologies of different \gls{dp} methods is their lack of a universal terminology for recognized element types. For example, a paragraph gets classified as \lstinline!NarrativeText! by the Unstructured.io framework \autocite{unstructuredio}, while Docling \autocite{docling} names the same category as simply \lstinline!TEXT!. Additionally, some methods provide classifications, which are not provided by others. One example for this is the addition of a \lstinline!ref_text! element type in the MinerU implementation \autocite{mineru,opendatalab}, referring to an entry in a bibliography. To address these issues, we aggregate all element categories from the evaluated \gls{dp} implementations into a single collection, normalizing their categories into a unified terminology. We then provide mappings for each of the implementations to our universal categories. The full list of available ParsingResultTypes is provided in \autoref{tab:parsing-result-type}.

\subsection{ParsingBoundingBox}
ParsingBoundingBox (\autoref{fig:parsing-bounding-box}) serves as the fundamental data type to denote the location of an entity in the document. It expands upon a bounding box in \gls{ltrb} format through the addition of a page number to support multi-page documents. The coordinates are stored as normalized fractional values of the page dimensions. Additionally, the data type includes the recursive attribute \lstinline!spans!, which enables the assignment of bounding boxes of higher granularity, such as individual text lines.

\begin{figure}[htpb]
  \centering
  \input{figures/code/parsing_bounding_box}
  \caption[ParsingBoundingBox]{Python implementation of the ParsingBoundingBox data type.}\label{fig:parsing-bounding-box}
\end{figure}

\subsection{ParsingResult}
Inspired by the data structures of multiple \gls{dp} implementations, such as Docling \autocite{docling_toolkit} and Google Document AI LayoutParser \autocite{layout_parser}, we choose a tree structure to represent the output of the parsing module. This approach has the benefit of being able to model the structure and hierarchies of the structural elements through parent-child relationships, ensuring a lossless representation of the original document. The ParsingResult data type (\autoref{fig:parsing-result}) represents a node inside of this tree structure.

\begin{figure}[htpb]
  \centering
  \input{figures/code/parsing_result}
  \caption[ParsingResult]{Python implementation of the ParsingResult data type.}\label{fig:parsing-result}
\end{figure}

The data type encapsulates all attributes identified during \gls{dp}. Its classification and bounding box, which are identified through \gls{dla}, are stored in the \lstinline!type! and \lstinline!geom! fields respectively. The latter also permits multiple bounding boxes for a single structural element, to allow for more flexibility regarding the localizations returned by the \gls{dp} implementation. The element's content, identified during content extraction, is stored in the \lstinline!content! field. Some implementations also persist images of figures or tables to the file system during content extraction, with \lstinline!image! containing their respective paths. The \lstinline!id! contains a document-wide unique identifier for the ParsingResult node. Lastly, \lstinline!parent! and \lstinline!children! model the tree structure.

The root node of the ParsingResult tree structure contains additional metadata about the parsing process, such as the elapsed parsing time, the used \gls{dp} implementation, or the path to the parsed \gls{pdf} document, in the \lstinline!metadata! field. Traversing the tree from the root node in a depth-first manner iterates through the elements in reading order.

\subsection{ChunkingResult}
The ChunkingResult (\autoref{fig:chunking_result}) is the final output of the document segmentation pipeline. It provides a wrapper around the list of generated chunks, adding a \lstinline!metadata! field for information about the input document and the document segmentation process. Hereby, the ChunkingResult combines both information about the chunking process, such as the chosen chunking strategy, as well as the metadata from the root node of the preceding ParsingResult tree.

\begin{figure}[htpb]
  \centering
  \subfloat[]{\input{figures/code/chunking_result}\label{fig:chunking_result}}
  \qquad
  \subfloat[]{\input{figures/code/chunk}\label{fig:chunk}}
  \caption[ChunkingResult]{Python implementation of the ChunkingResult \protect\subref{fig:chunking_result} and Chunk \protect\subref{fig:chunk} data types.}\label{fig:chunk_implementations}
\end{figure}

\subsection{Chunk}
The Chunk (\autoref{fig:chunk}) represents a singular passage used for the creation of the knowledge base in downstream \gls{rag} applications. In addition to the textual content of the chunk, which is stored in \lstinline!content!, the data type contains the ParsingBoundingBoxes required for visual source attribution. Lastly, the metadata field contains additional information about the chunk, such as its token length.

\section{Parsing Module}
The parsing module revolves around extracting the content of a raw \gls{pdf} file as a tree of ParsingResult nodes. It serves as an abstraction layer on top of various available \gls{dp} implementations, unifying different \gls{dp} approaches and output formats into a common interface. The module processes incoming documents through four sequential steps.

\paragraph{\gls{dp} interaction:}
In the first step, the module handles the interaction with the underlying \gls{dp} implementation. This includes request construction, preparing the input document in the required format of the implementation, and error handling. This logic is encapsulated through the abstract \lstinline!_parse! function, extracting the result of the \gls{dp} operation in the implementation-specific data structure.

\paragraph{Format conversion:}
Converting the implementation's data structure into the ParsingResult tree structure is the most crucial step to facilitate direct comparisons between different \gls{dp} approaches. Due to the significant variations in implementations' data structures, this step is the most complex task of the parsing module, with its specific inner workings and required steps being highly dependent on the implementation's data representations. Typical steps include the transformation of the element's bounding box coordinates into the normalized coordinates of the ParsingBoundingBox, the normalization of the elements classification into ParsingResultType, and the modeling of recognized hierarchical relationships in the ParsingResult tree. The abstract \lstinline!_transform! function encapsulate this implementation-specific conversion logic.

\paragraph{Parsing post-processing:}
Even after the normalization to the universal ParsingResult tree structure, there are still some inherent differences between the outputs from the different \gls{dp} approaches. In order to mitigate these differences and prepare the data for the chunking module, various rule-based post-processing steps are performed on the ParsingResult tree.

\begin{enumerate}
  \item \textbf{Element filtering:} Most documents contain textual information that does not belong to the main content of the document, such as page numbers and repeating page headers or footers. Some \gls{dp} implementations, such as MinerU \autocite{mineru}, already remove these elements in their own post-processing stages. We remove any elements that belong to non-main content element types as well as any textual elements with empty content. Specifically we remove all elements from the following types: \lstinline![REFERENCE_LIST, REFERENCE_ITEM, PAGE_FOOTER, PAGE_HEADER, FORM_AREA, WATERMARK]!. This reduces structural bias in the comparison between \gls{dp} implementations while removing unneeded information ahead of the chunking phase.

  \item \textbf{Hierarchy inference:} In order to represent the document's hierarchy as a tree structure, the relationships between the different elements need to be established. However, many of the evaluated \gls{dp} implementations do not return their output in a tree structure directly and instead provide a list of document elements in reading order. Other implementations, such as Docling \autocite{docling}, contain some hierarchy, such as the relationships between tables and their constituent table cells, while missing the relationships between section headings and the content belonging to their section. Inspired by the section heading matching process employed by Docling's HybridChunker \autocite{docling_toolkit}, we use the reading order of the document as well as the identified levels of the section headings to identify relationships between section headings and the nodes belonging to the section's content. This process is crucial in order to fully model the inherent structure of the document, which is a central prerequisite for enabling the creation of discourse passages in the chunking module.

  \item \textbf{Span-level bounding box identification:} In order to provide visual source attribution on a granularity higher than the ParsingResult level, more granular bounding boxes are needed. We use PyMuPDF \autocite{pymupdf}, a Python library for the extraction and analysis of data from \gls{pdf} documents, in order to extract the bounding boxes of individual lines of text inside of the bounding boxes of each ParsingResult. PyMuPDF enables the extraction of these bounding boxes either directly from the programmatic information contained in the \gls{pdf} file or through the use of \gls{ocr}. While we experimented with the use of both, due to the born-digital nature of the oncology guidelines, \gls{ocr} did not show any improved accuracy while being substantially slower than programmatic text extraction. If lines contain excessive horizontal whitespace, PyMuPDF tends to split them into separate bounding boxes. We address this by merging span bounding boxes if their vertical overlap is larger than a set threshold, resulting in unified bounding boxes for each line. For each element the identified span bounding boxes are stored in the \lstinline!span! field of their respective ParsingBoundingBox.
\end{enumerate}

\paragraph{Persistence:} To enable the evaluation of the output quality of the \gls{dp} implementations, the tree structure is serialized and persisted to the file system. Particularly, this includes two distinct serializations. Firstly, the content of the document is persisted as a lossy serialization to Markdown format. This format is used specifically for benchmarking the quality of the content extraction and is extracted directly from the implementation's data structure through the \lstinline!_get_md! function to ensure optimal adherence to the Markdown syntax. Some \gls{dp} solutions require additional processing for this extraction. The second persisted file contains the lossless \gls{json} serialization of the ParsingResult tree. Before serialization, metadata about the parsing process is added to the root node of the tree. This file is particularly important for evaluating the \gls{dla} capabilities of the \gls{dp} implementations.

\begin{table}[htpb]
  \centering
  \input{figures/tables/parsing_module_func.tex}
  \caption[Abstract Functions of the Parsing Module]{Overview over the abstract functions of the parsing module. Custom data format refers to the data types used by underlying \gls{dp} implementation.}
\end{table}\label{tab:parsing-functionality}

We provide integrations for eight different \gls{dp} implementations, which we will introduce in the following sections. However, the core principle of the parsing module lies in its extendibility. In order to incorporate an additional implementation into the parsing module, the abstract functions described in \autoref{tab:parsing-functionality} need to be implemented.

\subsection{Unstructured.io}
Unstructured.io is a prominent provider for \gls{dp}, offering both a cloud-based API as well as an open-source library. For our study, we will focus on the open-source library version of Unstructured.io \autocite{unstructuredio,unstructured_open_source}. While the developers themselves explicitly highlight that the open-source library is not suited for large-scale production environments \autocite{unstructured_open_source}, its inclusion within the documentation of popular \gls{rag} frameworks, such as Langchain \autocite{langchain} and LlamaIndex \autocite{llamaindex} make it a popular choice for a first point of contact with \gls{dp}. Therefore, we will regard the open-source library as a baseline for the compared implementations. Unstructured.io follows a modular pipeline approach. Specifically, the implementation uses YOLOX, a uni-modal vision transformer, to perform \gls{dla} \autocite{unstructured_open_source,yolox}. The library also includes a specialized model for table structure recognition \autocite{unstructured_open_source}.

\subsection{Docling}
Docling, which was developed by IBM in 2022, is one of the most popular available open-source \gls{dp} libraries \autocite{docling,docling_toolkit}. Docling particularly stands out from other \gls{dp} implementations through its permissive MIT license. To achieve this, Docling relies primarily on custom models instead of using third-party software, which is often not as permissive \autocite{docling}. Docling offers two different approaches for \gls{dp}:

\paragraph{Parsing pipeline:} Docling's processing pipeline consists of three components: a \gls{pdf} backend called DoclingParse, an internal model pipeline containing multiple \gls{ai} models, and a post-processing stage \autocite{docling_toolkit,docling}. Firstly, the \gls{pdf} backend extracts useful information from the document using both programmatic extraction as well as \gls{ocr} techniques. This includes bounding boxes for every text element inside the document. The internal model pipeline then performs both the \gls{dla} as well as content extraction steps. Hereby, Docling provides their own models for table structure recognition with the TableFormer model \autocite{docling_table_former} as well as for \gls{dla} with their Heron model \autocite{docling_heron}. Heron is derived from RT-DETR \autocite{rt_detr}, a uni-modal vision transformer, and retrained on DocLayNet \autocite{doclaynet}, Docling's own dataset for \gls{dla}. During \gls{dla}, identified bounding boxes are compared and intersected with bounding boxes retrieved from the \gls{pdf} backend in order to provide more accurate localization \autocite{docling_toolkit}. Using TableFormer, Docling is the only evaluated open-source system, that provides individual content and bounding boxes for table cells. During post-processing the recognized elements are then combined into the DoclingDocument data type \autocite{docling_toolkit}.

\paragraph{Granite Docling:} Granite Docling is an end-to-end \gls{vlm} for \gls{dp}. It belongs to the group of domain-specific \gls{vlm} models, specifically build for document understanding and conversion \autocite{granite_docling}. The model is very compact, consisting of around 258 million parameters \autocite{smol_docling,granite_docling}. With this model, Docling proposes the DocTags data format, a structured data format designed for representing both text and structure of the document through \gls{xml}-style tags \autocite{smol_docling}.

\subsection{MinerU}
Another popular choice for open-source on-device \gls{dp} is the MinerU framework. Similar to Docling, MinerU also offers both a pipeline as well as a \gls{vlm}-based approach for \gls{dp} \autocite{mineru,mineru_vlm,opendatalab}.

\paragraph{Parsing pipeline:} MinerU extends the traditional processing pipeline through a pre- and post-processing stage. In the pre-processing stage unprocessable files are filtered out and metadata about the document is extracted using the PyMuPDF library \autocite{mineru,pymupdf}. This metadata includes the language of the document, the document's page dimensions and the identification of scanned documents \autocite{mineru}. The pipeline then uses models from the \gls{dp} model library PDF-Extract-Kit for \gls{dla} and content extraction \autocite{mineru,opendatalab,doclayout_yolo,unimernet}. The model used for \gls{dla} is a fine-tuned version of LayoutLMv3, a multi-modal model \autocite{mineru,layoutlm_v3}. For content extraction, special models for formula and table structure recognition are employed by the pipeline. During the final post-processing stage, overlapping elements are cleaned up, unneeded elements are filtered out and the reading order of the document elements is inferred using a segmentation algorithm \autocite{mineru}. MinerU's pipeline system is the only evaluated implementation that includes span-level bounding boxes in its output, removing the need for their identification during post-processing.

\paragraph{\gls{vlm}:} With MinerU2.5, the implementation's offerings were expanded by a multi-stage \gls{vlm}-based \gls{dp} approach. This approach employs a 1.2 billion parameter \gls{vlm} to perform \gls{dp} in a two-stage approach \autocite{mineru_vlm}. Firstly, the model is used to perform \gls{dla} on the document, identifying elements and their reading order. In the second stage, the same model is applied again on individual image crops of the page element and is tasked to extract the content from the crop \autocite{mineru_vlm}.

\subsection{Gemini 2.5 Flash}
Gemini 2.5 Flash is a closed-source proprietary model developed by Google with strong multi-modal capabilities across text, vision and audio \autocite{gemini_2.5}. While Google offers a more capable model in the form of Gemini 2.5 Pro, we follow the sentiment from \textcite{mineru_vlm}, that \gls{dp} tasks \textquote[p.7]{typically exhibit relatively low dependency on large-scale language models} and, based on both models similar results on various image understanding benchmarks \autocite{gemini_2.5}, instead opt to rely on the cheaper, faster Gemini 2.5 Flash model for our study. Gemini 2.5 Flash belongs to the group of general-purpose \glspl{vlm} and, due to its closed-source nature, is only accessible through an \gls{api}. The Gemini family of models received additional training in order to provide improved accuracy on object detection and image segmentation tasks \autocite{gemini_2.5,gemini_image_understanding}. We follow the documentation provided by Google on harnessing Gemini's image understanding capabilities \autocite{gemini_image_understanding} to formulate a prompt, that takes advantage of this additional training for the \gls{dp} task. The full prompt is available in \autoref{lst:gemini-prompt}.

\subsection{LlamaParse}
LlamaParse is a cloud-based paid \gls{dp} service from the makers of LlamaIndex, a popular framework for building \gls{rag} systems and workflows \autocite{llamaindex,llamaparse}. While there is no official information on the architecture used for the \gls{dp} system behind LlamaParse, its marketing as a \textquote{GenAI-native document parser} \autocite{llamaparse} as well as the option to provide custom prompts to the service suggests that at least some of its functionality stems from a \gls{vlm}.

\subsection{Google Document AI LayoutParser}
LayoutParser from Google Document \gls{ai} is another cloud-based paid provider of \gls{dp} services \autocite{layout_parser}. Contrary to other services such as Google Document \gls{ai}'s Enterprise Document \gls{ocr} \autocite{google_ocr}, LayoutParser has a strong focus on identifying the relationships between different page elements. As such, LayoutParser can recognize the level of section headings, infer the hierarchy between different elements and extract the content from individual table cells. LayoutParser follows a multi-stage pipeline approach to perform \gls{dp}, but, as LayoutParser is a proprietary system, its exact architecture is unknown.

\section{Chunking Module}
The chunking module transforms the hierarchical ParsingResult tree into a sequence of chunks. We propose a novel solution aimed at increasing the traceability of the chunk content to its constituent ParsingResults, therefore enabling visual source attribution in the downstream \gls{rag} system.

Prominent implementations of chunking strategies, such as the ones found in the \gls{rag} frameworks LlamaIndex \autocite{llamaindex} and Langchain \autocite{langchain}, treat the chunking process as a division of the textual content of the document, typically in the form of a Markdown representation. This approach severs the link between the text and its underlying structural elements, complicating source attribution. Novel solutions, such as Docling's hybrid and hierarchical chunkers \autocite{docling,docling_toolkit}, improve upon this by associating the resulting chunk with a list of constructing structural elements. However, this inclusion is binary, with no distinction between partially and fully included elements. This results in bounding boxes that always contain the entire structural element, regardless of how much of its text is included in the content of the chunk. Furthermore, while these methods identify discourse passages based on the relationships between the ParsingResult nodes, the resulting chunks lack positional information from included section headings.

To address these limitations, we propose a token-centric architecture, enabling the traceability of the chunks content to its constituent ParsingResults on the token level. We argue that since \glspl{llm} and encoders operate on tokens rather than characters, the token serves as the atomic unit of textual content.

\begin{figure}[htpb]
  \centering
  \input{figures/code/rich_token.tex}
  \caption[RichToken]{Python implementation of the RichToken data type.}\label{fig:rich-token}
\end{figure}

The key concept of our proposed approach lies in the introduction of the RichToken (\autoref{fig:rich-token}). This data structure contains both the textual \lstinline!content! of the \lstinline!token! as well as its origin in the ParsingResult tree. The RichToken is linked to its ParsingResult node through its \lstinline!element_id!, the document-wide identifier of the ParsingResult. Its position inside the element's content is encapsulated in the \lstinline!token_idx! field.

\paragraph{Chunk Token Identification:}
The chunking process begins with the traversal of the document tree and the identification of RichToken groups that make up the resulting chunks. The specific logic for grouping these tokens, and therefore the type of the returned passages, is determined by the specific chunking strategy. As the module iterates through the ParsingResult nodes, their content is transformed into a stream of RichTokens using the \lstinline[language=HTML]!all-MiniLM-L6-v2! sentence transformer \autocite{sbert} as a tokenizer. To preserve the structural boundaries of the document tree, newline delimiters are placed between ParsingResult nodes, acting as textual representations of the document's structure. As the strategy traverses the tree, identified RichToken groups are sequentially emitted through Python's \lstinline[language=HTML]!yield! functionality. In addition, to avoid creating chunks that are too big for the encoder's context window, a limit $N$ for the maximum amount of tokens per chunk can be set on the chunking module.

\paragraph{Chunk Assembly:}
As the generator yields the identified RichToken groups, they are consumed by the Chunk assembly phase, constructing the final Chunk objects. Through the grounding provided by the RichTokens, the system identifies the included token ranges for the constituent ParsingResults. If only a partial number of the node's tokens are included in the chunk, only the relevant span bounding boxes are included in the chunk's positional information. We identify these spans by approximating the line that a token lies in, assuming constant token density through the content of the ParsingResult node. We find that by including the preceding and following lines of this approximation, inconsistencies from this approximation can be effectively mitigated, while still providing bounding boxes at a high granularity. Finally, the content of the chunk is aggregated and metadata, such as the chunk's token length, is stored in its respective field.
\\ \\
Our proposed chunking architecture enables precise visual source attribution for established chunking strategies, while providing structural information that the strategies can leverage during segmentation. We also address the limitations of previous visual source attribution systems \autocite{visa}, by enabling attributions to span over multiple pages. We provide implementations for four commonly used chunking strategies, with the architecture of the chunking module allowing the integration of additional strategies for future experiments.

\subsection{Fixed-Size Chunking}
Fixed-size chunking implements the window passage approach. It splits the document into chunks of the chunking module's maximum chunk length $N$, disregarding logical boundaries in favor of uniform chunk sizing \autocite{llamaindex_chunking}. The strategy traverses the ParsingResult tree in reading order, creating a queue of the document's RichTokens in the progress. When the queue reaches a length greater than $N$, the queue's first $N$ tokens are emitted and assembled into a chunk. In order to maintain some contextual continuity between the chunks, we implement a sliding-window mechanism with an overlap of $O$ tokens. This mechanism can lead to improved recall during the retrieval phase of downstream \gls{rag} systems \autocite{semantic_chunking,chunking_comparison}. After the chunk is emitted, only the first $N-O$ tokens are removed from the queue, leaving $O$ tokens to form the beginning of the subsequent chunk. After traversing the ParsingResult tree, any residual tokens are grouped together to form a final undersized chunk.

\subsection{Recursive Character Chunking}
Recursive character chunking leverages the structure of the textual content of the ParsingResult nodes to identify discourse paragraphs inside the document. The approach functions similarly to fixed-size chunking, however recursive character chunking utilizes an hierarchical list of delimiters (e.g., paragraphs, sentences, words) to define chunk boundaries \autocite{chunking_comparison,langchain_splitting_recursively,langchain}.

The delimiters used in this implementation are adapted from LangChain's RecursiveCharacterTextSplitter \autocite{langchain,langchain_splitting_recursively}, with punctuation added to better identify sentence endings, as suggested by \textcite{chroma_eval}. This results in the following delimiters: \lstinline|["\n\n", "\n", ".", "!", "?", " ", ""]|.

Once the queue's length exceeds $N$, the strategy splits the tokens using the highest-order delimiter. If there still exists a split which is larger than $N$, the process recurses on the oversized split with the next delimiter in the list. This ``coarse-to-fine'' approach preserves logical groupings while avoiding unnecessary fragmentation \autocite{chunking_comparison}. Similar to fixed-size chunking, recursive character chunking also incorporates a sliding window approach with an overlap of $O$ tokens between adjacent chunks.

\subsection{Breakpoint-based Semantic Chunking}
Breakpoint-based semantic chunking separates the document at the sentence level, inserting breakpoints in between sentences to denote chunk borders \autocite{semantic_chunking}. Instead of relying on structural markers, the strategy generates semantic passages by identifying shifts in the topics of the document.

As the strategy traverses the document tree, ParsingResult nodes are split into individual sentences using the punkt tokenizer from the \gls{nltk} \autocite{punkt_tab,nltk}. The strategy then computes their vector embeddings and the cosine distance of each adjacent sentence pair using the \lstinline[language=HTML]!all-MiniLM-L6-v2! encoder. A high distance, which is the inverse of the sentences similarity, denotes a topical shift between these sentences \autocite{langchain_splitting_recursively}.

We then insert a breakpoint between sentences which have a higher distance than the $Q$-th percentile of all calculated distances. Breakpoint-based semantic chunking has no regard for the size of its produced chunks, leading to a large variability in chunk sizes. To mitigate this issue, we introduce a minimum chunk size $M$. If the strategy produces a chunk which is shorter than $M$, we combine it with the next splits until their combined length is greater than $M$. To handle the inverse problem, we use the strategy of recursive character chunking in order to further split oversized chunks.

\subsection{Hierarchical Chunking}
Hierarchical chunking, which is inspired by the hybrid chunker from the Docling toolkit \autocite{docling_toolkit}, generates discourse passages by leveraging the tree structure of the ParsingResult. The structure and hierarchy of the document are preserved in the final chunks by prepending relevant section headers to the chunk's content.

In order to prevent deep hierarchical structures from taking up a large part of the final chunk, a token budget $B_\text{headings}$ is imposed on the length $S$ of the section heading tokens. If adding an additional heading causes $S$ to exceed $B_\text{headings}$, the highest-level ancestors are removed from the list until the size constraint is satisfied.

As the algorithm traverses the document tree, it processes each ParsingResult node following a three-step logic:
\begin{enumerate}
  \item \textbf{Subtree evaluation:} The algorithm determines the token count of the entire subtree rooted at the current node. If the length of the subtree is less than the remaining capacity, $N-S$, the subtree is grouped together into a single chunk. This prevents unnecessary fragmentation of small structures inside the document.

  \item \textbf{Recursion:} If the subtree exceeds the size limit the node's content is appended to the heading tokens and the algorithm recurses onto the children of the node. After recursion, adjacent children nodes are merged as long as they were not split any further during recursion and their combined length does not exceed $N-S$. This ensures that resulting chunks are as close to the length $N$ as possible. After merging, the content's tokens are prepended to each of the splits and the splits are returned.

  \item \textbf{Leaf-splitting:} If the algorithm reaches a leaf node that exceeds the available space $N-S$, it results to using recursive character splitting to determine the chunk boundaries. Following the logic from the recursion step, the content's tokens are prepended to the splits before they are returned.
\end{enumerate}

\section{Evaluation Framework}

\subsection{Document Layout Analysis Evaluation}
The goal of the \gls{dla} evaluation is to assess the correctness of the bounding boxes and type labels produced by the parsing module \autocite{icdar2009}. While there are multiple datasets available for this task \autocite{docbank,doclaynet,omnidocbench}, we will use the PubLayNet dataset \autocite{publaynet} for our evaluation. While many datasets focus on evaluating \gls{dla} on a range of different document types such as forms, invoices or handwritten documents, PubLayNet consists solely of medical scientific articles \autocite{omnidocbench,publaynet}. This format closely resembles the format of the oncology guideline documents which makes it a suitable choice for this evaluation. Compromised of over 360.000 automatically annotated document pages collected from \gls{pmcoa}, PubLayNet is one of the largest datasets for \gls{dla} \autocite{publaynet}. As the dataset in its entirety is no longer publicly available and far too large for the purposes of this thesis, we will use \lstinline!publaynet-mini!, a small subset of 500 pages of the original dataset for this evaluation \autocite{publaynet-mini}. As seen in \autoref{tab:publaynet_counts}, the subset contains around 5000 ground truth annotations for elements from 5 different classes.

\begin{table}[htpb]
  \centering
  \input{figures/tables/publaynet_counts}
  \caption{Distribution of ground truth annotations across the different element types contained in the \lstinline!publaynet-mini! subset of the PubLayNet dataset.}\label{tab:publaynet_counts}
\end{table}

To assess the performance of different predictors on object detection tasks such as \gls{dla}, \gls{ap} is the most commonly used metric \autocite{object_detection_survey}. Previous evaluations of \gls{dla} models on the PubLayNet dataset also use a version of this metric \autocite{icdar2021_competition}.\ However, \gls{ap} relies on the predictor's confidence values, indicating how confident the predictor is about a predicted bounding box and class label. As most of the \gls{dp} implementations provide ``hard-predictions'', which do not contain any confidence values, the \gls{ap} is not a viable metric for the purposes of this study \autocite{lrp_error,eclair}.

For this reason, we will compare the implementations based on their achieved F1 score. Similarly to \gls{ap}, this metric takes into account two important measures for object detectors: precision and recall \autocite{object_detection_metrics}. According to \textcite{object_detection_metrics}, \textquote[p. 9]{Precision is the ability of a model to identify only relevant objects.\ \textelp{} Recall is the ability of a model to find all relevant cases \textelp{}}. In order to calculate their values, firstly the \glspl{dtbb} are classified into \glspl{tp} and \glspl{fp}. A \gls{dtbb} is classified as a \gls{tp} if there exists a \gls{gtbb} from the same class, so that their \gls{iou} is greater than a given threshold. One \gls{gtbb} can not be matched to multiple \glspl{dtbb}. If there does not exist a \gls{gtbb} that fulfils these criterions, the \gls{dtbb} is classified as a \gls{fp}. Any \glspl{gtbb} which were not matched to a \gls{dtbb} are classified as \glspl{fn}. Following the definition from \textcite{object_detection_metrics} for a model that, on a dataset with $G$ \glspl{gtbb}, outputs $N$ \glspl{dtbb}, out of which $S, (S\leq N)$ are \glspl{tp}, precision and recall can be formulated as shown in \autoref{eq:precision} and \autoref{eq:recall}.

\begin{equation}
  \text{Pr} = \frac{\sum_{n=1}^{S}\text{TP}_{n}}{\sum_{n=1}^{S}\text{TP}_{n} + \sum_{n=1}^{N-S}\text{FP}_{n}} = \frac{\sum_{n=1}^{S}\text{TP}_{n}}{all\ detections}
  \label{eq:precision}
\end{equation}

\begin{equation}
  \text{Re} = \frac{\sum_{n=1}^{S}\text{TP}_{n}}{\sum_{n=1}^{S}\text{TP}_{n} + \sum_{n=1}^{G-S}\text{FN}_{n}} = \frac{\sum_{n=1}^{S}\text{TP}_{n}}{all\ ground\ truths}
  \label{eq:recall}
\end{equation}

The F1 score is the weighted harmonic mean between precision and recall and is calculated as defined in \autoref{eq:f1score} \autocite{object_detection_metrics}. The F1 score is calculated for a single class at a set \gls{iou} threshold. Selecting a higher threshold will lead to a stricter metric as predictions need to be more precise to be counted as a \gls{tp} \autocite{object_detection_metrics}. A F1 score calculated at an \gls{iou} threshold T\% is commonly referred to as F1@T \autocite{scenescript}.

\begin{equation}
  F_1 = 2\frac{\text{Pr}\cdot\text{Rc}}{\text{Pr}+\text{Rc}}
  \label{eq:f1score}
\end{equation}

For scenarios with multiple classes, such as the PubLayNet dataset, the Macro F1 score can be used to assess the overall performance of the predictor \autocite{maximize_f1}. The Macro F1 score is the mean of the single class F1 scores. For a dataset with $M$ different classes, the calculation of the Macro F1 score is described in \autoref{eq:macrof1}. Hereby, $N_{:j}$ and $G_{:j}$ denote the \glspl{dtbb} and \glspl{gtbb} belonging to elements of class $j$ \autocite{maximize_f1}.

\begin{equation}
  F_{1_\text{Macro}}(N, G) = \frac{1}{M}\sum_{j=1}^{M}F_1(N_{:j}, G_{:j})
  \label{eq:macrof1}
\end{equation}

The \gls{dp} implementations will be evaluated on both their single-class and Macro F1 scores. Specifically, their (Macro) F1@50 and F1@50:95 will be compared against each other. F1@50:95 refers to the mean of the F1 values calculated at 10 evenly spaced \gls{iou} thresholds between 0.5 and 1.0 and is inspired by the primary challenge metric found in the MS COCO dataset \autocite{coco}. This rewards implementations, which provide more accurate bounding boxes \autocite{coco}. F1@50 is chosen, as a threshold of 50\% is one of the most commonly used threshold values for metrics in object detection \autocite{object_detection_metrics}. To calculate these metrics, the \lstinline[language=HTML]!faster-coco-eval! package is used to determine the recall and precision values at the \gls{iou} thresholds \autocite{faster-coco-eval}.

\subsection{Content Extraction Evaluation}
The goal of content extraction evaluation is to evaluate the quality of the extracted textual content from the document. In the context of oncology guidelines, where an error in the extraction of textual content can have a fatal effect, potentially altering clinical recommendations, assessing the quality of the content extraction is a critical aspect of our evaluation.

OmniDocBench has established itself as the leading benchmark for performing end-to-end evaluations \autocite{omnidocbench,paddleocr,mineru_vlm,monkeyocr,dotsocr,docling_toolkit}, in particular for evaluating the quality of the \gls{dp} implementation's Markdown output. The benchmark includes nine different \gls{pdf} document types, such as academic literature, newspapers, and financial reports, from 3 different language types, Chinese, English, and mixed \autocite{omnidocbench}. In total, the benchmark contains 1355 single-page \gls{pdf} documents \autocite{omnidocbench}. For the purpose of this study, we limit our evaluation to english academic literature. These documents closely resemble the layout of the \glspl{cpg}, including both single and double-column layouts with complex tables and figures. After filtering, we are therefore left with a total of 129 \gls{pdf} documents for our evaluation.

The dataset behind OmniDocBench was created through a semi-automatic process. Initial annotations are retrieved using LayoutLMv3 \autocite{layoutlm_v3} for \gls{dla} and PaddleOCR \autocite{paddleocr}, UniMERNet \autocite{unimernet}, and GPT-4o \autocite{gpt4o} for content extraction. Annotators manually refine the extracted annotations, correcting reading order and extracted content, as well as affiliating captions with their respective figures and tables. In a final steps expert researchers review and correct mathematical formulas and tables to ensure accuracy in the final annotations. In total, the dataset includes over 20,000 annotated structural elements.

The evaluation of a single \gls{dp} implementation follows a three-step process:

\paragraph{Extraction:} In a first step, the documents' elements are extracted from the Markdown texts. Since the Markdown format is purely textual, this step is primarily performed using regular expression matching \autocite{omnidocbench}. The format of tables varies depending on the \gls{dp} implementation with Markdown, \gls{html} and \LaTeX{} formats being possible. To prevent interference between extraction steps, the extraction of different types follows a specific order. After extraction, Markdown tables are converted into \gls{html} format for further processing. In total, five different element types are extracted: \LaTeX{} and \gls{html} tables, display formulas, code blocks, and paragraphs \autocite{omnidocbench}.

\paragraph{Matching:} The extracted elements are now matched to ground truth elements of the same type through a process called Adjacency Search Match \autocite{omnidocbench}. First, the normalized edit distances between each possible pair is calculated, with pairs that exceed a specific threshold being considered as a successful match. As different implementations separate paragraphs at different positions, fuzzy matching is used to identify any paragraphs that are substrings of a respective matching partner. If so the substring paragraph is merged with its adjacent paragraphs until the pair's normalized edit distance starts to increase \autocite{omnidocbench}. This ensures that the way paragraphs are separated does not influence the matching process. Some element types, such as headers and figure captions, are automatically removed by some implementations. To ensure a fair comparison, these elements are also removed before the calculation of the metrics \autocite{omnidocbench}.

\paragraph{Metric calculation:} OmniDocBench provides a multitude of different evaluation metrics for the extracted element \autocite{omnidocbench}. Based on the characteristics of the oncology guidelines, we select the following subset of metrics for our evaluation:

\begin{enumerate}
  \item \textbf{Normalized Edit Distance:}
        We rely on the normalized edit distance to evaluate how well the \gls{dp} implementation extracts content from the textual elements of the oncology guidelines. This is a crucial step for ensuring that clinical recommendations remain intact and unaltered.

        The \gls{gld}, also known as the edit distance, is a metric that measures the textual similarity between two strings $X, Y \in\Sigma^*$, with $\Sigma^*$ being the set of strings over an alphabet $\Sigma$ \autocite{levenshtein,edit_distance}. $\lambda\notin\Sigma$ is the empty string. The metric denotes the minimum cost of transforming $X$ into $Y$ through weighted elementary edit operations. According to \textcite{edit_distance}, an \textquote[p.1]{elementary edit operation [$T$] is a pair $(a,b)\neq(\lambda,\lambda)$, often written as $a\to b$, where both $a$ and $b$ are strings of lengths 0 or 1.} There are three elementary edit operations: insertions ($\lambda\to a$), substitutions ($a\to b$), and deletions ($b\to\lambda$). The edit transformation of $X$ into $Y$, $T_{X,Y}=T_1T_2\dots T_l$, is a sequence of elementary edit operations. Using a weight function $\gamma$ which assigns a nonnegative real number $\gamma(T)$ to each elementary edit operation $T$, the weight of the edit transformation $T_{X,y}$ is computed as defined in \autoref{eq:trans_weight} \autocite{edit_distance}.

        \begin{equation}
          \gamma(T_{X,Y})=\sum_{i=1}^{l}\gamma(T_i)
          \label{eq:trans_weight}
        \end{equation}

        Following the definitions from \textcite{edit_distance}, given $X,Y\in\Sigma^*$, the \gls{gld} is then defined as in \autoref{eq:gld}.

        \begin{equation}
          \text{GLD}(X,Y)=\min\{\gamma(T_{X,Y})\}
          \label{eq:gld}
        \end{equation}

        The \gls{gld} is not normalized with respect to the lengths of $X$ and $Y$. A short string $X$ that requires the same elementary edit operations as a longer string $X', |X|<|X'|$ is therefore not additionally penalized. The normalized \gls{gld}, which is defined in \autoref{eq:ngld}, addresses this issue. For a weight function $\gamma$, that assigns the same weight to insertions and deletions of any character, $d_{\text{N-GLD}}$ is a metric over $\Sigma^*$ whose values are in $[0,1]$ \autocite{edit_distance}.

        \begin{equation}
          \begin{split}
            \alpha                & =\max\{\gamma(a\to\lambda),\gamma(\lambda\to b),a,b\in\Sigma\}      \\
            d_{\text{N-GLD}}(X,Y) & =\frac{2\cdot\text{GLD}(X,Y)}{\alpha\cdot(|X|+|Y|)+\text{GLD}(X,Y)}
          \end{split}
          \label{eq:ngld}
        \end{equation}

  \item \textbf{\gls{teds}:}
        As complex tables are a common occurrence in the oncology guidelines, examining the quality of the table structure recognition of the different implementations, is a key requirement for our evaluation.\ \gls{teds} measures the similarity between two \gls{html} tables. In order to calculate this metric, extracted \LaTeX{} tables are first converted into \gls{html} format \autocite{omnidocbench}.\ \gls{teds} builds on top of the tree edit distance proposed by \textcite{tree_edit_distance}. Hereby, the tree edit distance is defined as \textquote[p.1]{the minimum-cost sequence of node edit operations that transforms [a] tree $F$ into [another tree] $G$} \autocite{tree_edit_distance}.

        In \gls{html}, tables are represented as tree structures. The root node has two children, \lstinline!thead! and \lstinline!tbody!, grouping the table's header rows and body rows respectively. Each table row \lstinline!tr! is made up of table cells \lstinline!td!, the leaves of the table tree. A cell has three attributes. The attributes ``colspan'' and ``rowspan'' denote the respective number of columns and rows that the cell stretches across. Lastly, ``content'' includes the textual content of the table cell \autocite{teds}.

        \gls{teds} defines three different operations and their respective costs. Inserting or deleting a node has a cost of 1. The cost of substituting a table node $n_o$ with $n_s$ varies on the type and attributes of the node. If one of the node is not a leaf node their substitution, such as switching the order of two rows, has a cost of 1. When both nodes are table cells, their substitution cost is 1 if their spanning columns or rows are different. If the leaf nodes differ in their content, their substitution cost is the normalized \gls{gld} between their contents \autocite{teds}.
        Following the definition from \textcite{teds}, the \gls{teds} between two table trees $F$ and $G$ is then computed as in \autoref{eq:teds}. Hereby, $|T|$ denotes the number of nodes in a table tree $T$ and $\text{EditDist}$ is the tree edit distance. The value of \gls{teds} is confined to $[0,1]$ \autocite{teds}.

        \begin{equation}
          \text{TEDS}(F,G)=1-\frac{\text{EditDist}(F,G)}{\max(|F|,|G|)}
          \label{eq:teds}
        \end{equation}

        In addition to the regular \gls{teds}, OmniDocBench provides a structure-only version of this metric \autocite{omnidocbench}. This version disregards differences between the content of table cells, effectively setting the substitution cost of two table cells that span the same amounts of rows and columns to 0. In combination, these metrics help discern between the \gls{dp} approach's ability to understand the structure of the table and its ability to extract accurate textual information from the table cells.

  \item \textbf{Normalized edit distance for the reading order:}
        Evaluating the reading order of the implementation's output is an important validation step to ensure that the system is able to adapt to the double and single column layouts of the oncology guidelines. The normalized edit distance of the reading order is used for this evaluation \autocite{omnidocbench}. In order to create a fair comparison only text elements are included for this evaluation, as the placement of floating elements such as tables or figures can be somewhat ambiguous \autocite{omnidocbench}. Each of the ground truth elements contains an integer referring to their index in the natural reading order of the source document. The list of these indices is referred to as $I$. The indices are then ordered by the reading order of their matched predictions in the Markdown document. This list of reordered indices is referred to as $I'$ and denotes the ordering as returned from the \gls{dp} implementation. The normalized edit distance between $I$ and $I'$ is then calculated, with the elementary edit operations being the removal, insertion and substitution of a list element.
\end{enumerate}

The chosen metrics and document filters are stored in a configuration file and passed to the benchmark during evaluation. The full OmniDocBench configuration file that is used for our evaluation is displayed in \autoref{fig:omni-doc-config}.

\subsection{Chunking Evaluation}
The quality of the \gls{dp} process is meaningless if the relevant passages can not be found during the retrieval phase. As previously discussed, chunking can have a significant impact on the retriever's ability to find the correct passages. However, measuring the quality of chunking is not trivial and generally overlooked during evaluation \autocite{chroma_eval}. Established \gls{rag} evaluation frameworks, such as Ragas \autocite{ragas}, evaluate the quality of the retrieved context by using a \gls{llm} to decide whether a retrieved chunk is relevant to the query \autocite{ragas}. This approach not only introduces additional complexity and uncertainty but also fails to measure the ratio of relevant information inside the retrieved chunks.

\textcite{chroma_eval} propose a framework that focuses specifically on evaluating the influence of the chunking process on the retrieval phase. Their methodology builds on the fact that retrieving irrelevant tokens results in unnecessary computational strain and distractions during generation \autocite{llm_context_distractions,chroma_eval}. Therefore they propose a novel evaluation approach that evaluates the retrieved context on the token level. Their contributions include the proposal of specialized evaluation metrics as well as a framework for the creation of the evaluation dataset. We follow their evaluation approach while adapting it to our own data types.

\paragraph{Dataset creation:} To compare the chunking strategies against each other, a corpus of oncology guideline documents needs to be selected for the evaluation. In addition, \gls{qa} pairs are needed to evaluate the quality of the retrieval. Hereby the framework provides utilities for the generation of synthetic \gls{qa} pairs from the document corpus through the use of a \gls{llm}. However, for our evaluation we use manually annotated \gls{qa} pairs, created by medical professionals from the \gls{tum} university hospital. While this dataset is not available to the general public, we were permitted to use both the documents as well as the \gls{qa} pairs for this evaluation. In total, the dataset contains \todo{count about 30?} \gls{qa} pairs created for a document corpus of \todo{count} german oncology guidelines published by \gls{awmf}.

The documents are first processed by the parsing module to prepare them for the chunking strategies. Hereby, we choose the used \gls{dp} implementation based on the results of the content extraction evaluation. The document corpus $D$ contains the tokens across all of the parsed guideline documents. The parsed documents are then processed by the chunking module once for each chunking strategy. For each strategy $i$, the chunks across all of the documents are contained in the chunk corpus $C_i$. Each chunk $c\in C_i$ is a set of tokens such that $c\subseteq D$. For each question $q$, the answer subset $T_e,T_e\subseteq D$ contains the information that is relevant for answering $q$.

\paragraph{Evaluation:} \textcite{chroma_eval} argue that a metric for measuring the quality of the retrieval phase should \textquote{take into account not only whether relevant excerpts are retrieved, but also how many irrelevant, redundant, or distracting tokens are \textelp{} retrieved}. During the evaluation phase a retriever is created for each chunk corpus $C$ and its performance is evaluated using the \gls{qa} pairs.

Based on the question $q$, the retriever retrieves a set of chunks $c=\{c_1,c_2,\dots,c_l\}, c_i\subseteq D, c_i\in C$ from the chunk corpus $C$. $T_r,T_r=\bigcup_{c_i\in c}{c_i}$ is the set of all tokens contained in the retrieved chunks. As $T_r$ is a set, it does not contain any duplicate tokens \autocite{discrete_math}. However, for many chunking strategies that employ a sliding-window approach, the same token might be included in multiple retrieved chunks. Retrieving the same token twice introduces additional noise, which should be penalized by the evaluation metrics.

For the metric calculation, \textcite{chroma_eval} account for this redundancy by noting that the cardinality of $T_r$ includes the multiplicity of included tokens \autocite{chroma_eval}. To formulate this logic with the needed mathematical rigor, we define $M_r$ as the multiset over $T_r$. $M_r:T_r\to \mathbb{N}_0$ is a function such that if $M_r(t)=k>0, t\in T_r$, then $t$ appears with multiplicity $k$ in $M_r$ \autocite{discrete_math}. The cardinality of the multiset is defined as $|M_r|=\sum_{t\in T_r}{M_r(t)}$ \autocite{discrete_math}. Through $|T_r|$ and $|M_r|$, we can express both the number of unique retrieved tokens and the number of total retrieved tokens, including duplications.

Following this principle, \textcite{chroma_eval} propose three distinct evaluation metrics. For each evaluated chunking strategy, the mean of each metric over the \gls{qa} pairs is reported \autocite{chroma_eval}.

\begin{enumerate}
  \item \textbf{Token-wise precision:} Analog to its definition in the \gls{dla} evaluation, the token-wise precision measures the ratio of retrieved tokens that are \glspl{tp}, meaning relevant for the question. Hereby, returning a relevant token twice leads to a lower precision. Given a query $q$ and a chunked document corpora $C$, the token-wise precision is calculated as in \autoref{eq:precision_token} \autocite{chroma_eval}.

        \begin{equation}
          \text{Pr}_q(C)=\frac{|T_e\cap T_r|}{|M_r|}
          \label{eq:precision_token}
        \end{equation}

  \item \textbf{Token-wise recall:} Token-wise recall measures how many of the relevant tokens were successfully retrieved. Given a query $q$ and a chunked document corpora $C$, the token-wise recall is then calculated as in \autoref{eq:recall_token} \autocite{chroma_eval}.

        \begin{equation}
          \text{Re}_q(C)=\frac{|T_e\cap T_r|}{|T_e|}
          \label{eq:recall_token}
        \end{equation}

  \item \textbf{Token-wise \gls{iou}:} Similar to the \gls{iou} between bounding boxes, the token-wise \gls{iou} calculates the overlap between the retrieved tokens and the ground truth tokens. Given a query $q$ and a chunked document corpora $C$, The token-wise \gls{iou} is calculated as described in \autoref{eq:iou_token} \autocite{chroma_eval}.

        \begin{equation}
          \text{IoU}_q(C)=\frac{|T_e\cap T_r|}{|T_e|+|M_r|-|T_e\cap T_r|}
          \label{eq:iou_token}
        \end{equation}

  \item \textbf{$\text{Precision}_\Omega$:} $\text{Precision}_\Omega$ refers to the precision value of a ``perfect'' retriever that always retrieves the set of chunks $c_\text{opt}$ consisting of every chunk that contains tokens from $T_e$. The metric therefore provides an upper bound for the token efficiency given perfect recall \autocite{chroma_eval}.
\end{enumerate}

% For our evaluation

% 3 token lengths: small 128, medium 256, large 512
% 2 overlaps: 50\% and 0\%
% minimum chunk size 50\% and 0\%
% b headings fixed at 25\%

% Total of 19 configurations




\subsection{Evaluation Environment}
All evaluations are performed on a 2023 MacBook Pro equipped with a M3 processor and 16 GB of unified memory. In order to optimize the performance of this hardware, especially for the locally deployed \gls{vlm}-based \gls{dp} approaches, we utilize Apple's MLX engine \autocite{mlx} when possible.