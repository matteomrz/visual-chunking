% !Tex root = ../main.tex

\chapter{Methodology}\label{chapter:methodology}

\section{Pipeline overview}
In order to be able to compare different \gls{dp} implementations and chunking strategies, we developed a modular document segmentation pipeline. The system transforms the raw \gls{pdf} documents into chunks through a two-stage process. Due to this modular approach, the pipeline allows to easily swap out both the used \gls{dp} implementation and chunking strategy, while maintaining a unified interface for both modules. The architecture of the data processing pipeline is illustrated in \autoref{fig:pipeline-overview}.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{pipeline_diagram}
  \caption[Document Segmentation Pipeline]{Overview of the document segmentation pipeline architecture. The pipeline consists of two main modules: the parsing module and the chunking module. The parsing module transforms unstructured \gls{pdf} documents into the structured ParsingResult format while extracting it's textual content as a Markdown file. The chunking module then splits the ParsingResult into smaller chunks suitable for \gls{rag} applications, while maintaining positional information for visual source attribution.}\label{fig:pipeline-overview}
\end{figure}

\textbf{Parsing module:} The parsing module is the first step of the pipeline. It provides a unified interface for eight different \gls{dp} implementations. Firstly, document elements and their structural information are extracted from the document using the underlying \gls{dp} implementation. The output then gets converted into a standardized data format and undergoes further post-processing steps, such as filtering unwanted element types, to prepare the data for the chunking module. Additionally, the structured data representation of the document gets persisted to the file system as a lossless JSON serialization as well as a lossy serialization to Markdown format.

\textbf{Chunking module:} The chunking module is responsible for the splitting of the structured data into smaller chunks using one of multiple available document chunking strategies. We adapt four established strategies to provide accurate positional information in the form of bounding boxes, enabling visual source attribution for each chunk. To achieve this we propose a novel approach to the document chunking paradigm, enabling traceability of the chunk's content on the token-level. Through this approach, we are able to determine more accurate chunk bounding boxes compared to established methods.

\section{Data representations}
As of the time of writing, every \gls{dp} implementation defines their own datatypes, making comparisons very complex. In order to consolidate multiple different \gls{dp} implementations into our modular pipeline and evaluate them against each other, the output of each implementation needs to be transformed into a uniform format. For this purpose, we define our own universal datatypes to be used in the data processing pipeline.

\subsection{ParsingBoundingBox}
The ParsingBoundingBox (\autoref{fig:parsing-bounding-box}) datatype adds additional fields to a traditional bounding box tuple. Since the guideline documents usually contain multiple pages, we include a \lstinline!page! field indicating the page number that the bounding box belongs to. The coordinates of the bounding box are saved as a normalized tuple in the $(x_{\min}, y_{\min}, x_{\max}, y_{\max})$ format. Additionally, the spans field allows for saving more granular bounding boxes such as boxes for every line of text inside the bounding box.

\begin{figure}[htpb]
  \centering
  \input{figures/code/parsing_bounding_box}
  \caption[ParsingBoundingBox]{Python implementation of the ParsingBoundingBox datatype.}\label{fig:parsing-bounding-box}
\end{figure}

\subsection{ParsingResultType}
One problem with comparing multiple \gls{dp} methods is their lack of universal categories. This leads to two specific issues.

Firstly, the naming of element types differs between implementations. For example, a paragraph gets classified as \lstinline!NarrativeText! by the Unstructured.io framework \autocite{unstructuredio}, while Docling \autocite{docling} names the same category as simply \lstinline!TEXT!.

Secondly, some methods provide classifications, which are not provided by others. One example for this is the addition of a \lstinline!ref_text! element type in the MinerU implementation \autocite{mineru,opendatalab}, referring to an entry in a bibliography. For our purposes, we aggregated all possible categories from the tested implementations and provided mappings to the universal types for each \gls{dp} implementation. The full list of ParsingResultTypes can be found in \autoref{tab:parsing-result-type}.

\subsection{ParsingResult}
The ParsingResult (\autoref{fig:parsing-result}) is the output of the parsing module. It is inspired by the DoclingDocument datatype from the Docling library \autocite{docling}. Mainly, ParsingResult adopts the tree structure proposed by multiple \gls{dp} implementations to represent hierarchical relationships between document elements.

\todo{Explain difference to doclingdoc: no different classes, easier bounding boxes, only tree structure with no group or text lists}

\begin{figure}[htpb]
  \centering
  \input{figures/code/parsing_result}
  \caption[ParsingResult]{Python implementation of the ParsingResult datatype.}\label{fig:parsing-result}
\end{figure}

The \lstinline!geom! attribute contains a list of bounding boxes related to the element. This allows elements, such as tables, to span multiple pages by containing multiple bounding boxes. Using the \lstinline!parent! and \lstinline!children! attributes of the ParsingResult, the hierarchy of the input document is represented as a tree of ParsingResult nodes. This way, hierarchical relationships between the elements can be represented. For example, a section header has all text paragraphs contained in it's section as children.

\subsection{ChunkingResult}
The ChunkingResult (\autoref{fig:chunks}) is the output of the chunking module. It provides a wrapper around a list of Chunks, adding a \lstinline!metadata! field for information about the document and the preceeding processes.

\begin{figure}[htpb]
  \centering
  \input{figures/code/chunking_result}
  \qquad
  \input{figures/code/chunk}
  \caption[ChunkingResult]{Python implementation of the ChunkingResult and Chunk datatypes.}\label{fig:chunks}
\end{figure}

\subsection{Chunk}

\section{Parsing module}
The parsing module processes an incoming \gls{pdf} document in a multi-step process. Firstly, the document is passed to the internal \gls{dp} implementation using the \lstinline!_parse! function. Through the \lstinline!_transform! function, the raw output is then converted into the ParsingResult format. Additionally, the \lstinline!_get_md! function extracts the Markdown representation of the document from the raw output of the internal parser. Finally, additional metadata about the parsing process, such as the duration of the internal parsing operation, are added to the root ParsingResult. After performing additional post-processing steps, both the Markdown as well as the lossless serialization of the ParsingResult in JSON output are then persisted to the file system. As the raw types returned by the internal parser differ based on the specific implementation, each of the abstract functions mentioned above need to be implemented by the specific parser class. An overview of the abstract functions to be implemented can be found in \autoref{tab:parsing-functionality}.

\begin{table}[htpb]
  \centering
  \input{figures/tables/parsing_module_func.tex}
  \caption[Parsing module abstract functions]{Abstract functions of the Parsing Module to be fulfilled by the implementation.}
\end{table}\label{tab:parsing-functionality}

\subsection{Unstructured.io}
Unstructured.io is a prominent provider for \gls{dp}, offering both a cloud-based API as well as an open-source library. For our study, we will focus on the open-source library version of Unstructured.io \autocite{unstructuredio,unstructured_open_source}. While the developers themselves explicitly highlight that the open-source library is not suited for large-scale production environments \autocite{unstructured_open_source}, it's inclusion within the documentation of popular \gls{rag} frameworks, such as Langchain \autocite{langchain} and LlamaIndex \autocite{llamaindex} make it a popular choice for a first point of contact with \gls{dp}. Therefore, we will regard the open-source library as a baseline for the compared implementations. Unstructured.io follows a modular pipeline approach. Specifically, the implementation uses YOLOX, a unimodal vision transformer, to perform \gls{dla} \autocite{unstructured_open_source,yolox}. The library also includes a specialized model for table structure recognition \autocite{unstructured_open_source}.

\subsection{Docling}
Docling, which was developed by IBM in 2022, is a one of the most popular available open-source \gls{dp} libraries \autocite{docling,docling_toolkit}. Docling particularly stands out from other \gls{dp} implementations through it's permissive MIT license. To achieve this, Docling relies primarily on custom models instead of using third-party software, which are often not as permissive \autocite{docling}. Docling offers two different approaches for \gls{dp}:

\textbf{Parsing pipeline:} Docling's processing processing pipeline consists of three components: a \gls{pdf} backend called DoclingParse, an internal model pipeline containing multiple \gls{ai} models, and a post-processing stage. Firstly, the \gls{pdf} backend extracts useful information from the document using both contained programmatic information as well as \gls{ocr} techniques. This includes bounding boxes for every text element inside the document. The internal model pipeline then performs both the \gls{dla} as well as content extraction steps. Hereby, Docling provides their own models for table structure recognition with the TableFormer model as well as for \gls{dla} with their Heron model \autocite{docling_heron}. Heron is derived from RT-DETR \autocite{rt_detr}, a unimodal vision transformer, and retrained on DocLayNet \autocite{doclaynet}, Docling's own dataset for \gls{dla}. During \gls{dla} identified bounding boxes are compared and intersected with bounding boxes retrieved from the \gls{pdf} backend in order to provide more accurate localization \autocite{docling_toolkit}. During post-processing the recognized elements are then combined into the DoclingDocument datatype \autocite{docling_toolkit}.

\textbf{Granite Docling:} Granite Docling is an end-to-end \gls{vlm} for document parsing, released in \todo{date}.

Both methods return their output in the DoclingDocument format. DoclingDocument

\subsection{MinerU}
Another popular choice for open-source on-device \gls{dp} is the MinerU framework. Similar to Docling, MinerU also offers both a pipeline as well as a \gls{vlm} based approach for \gls{dp} \autocite{mineru,mineru_vlm}. \\ \\
\textbf{Parsing pipeline:}

\subsection{Google Gemini}

\subsection{LlamaParse}

\subsection{Google Document AI Layout Parser}

\section{Post-processing methods}

\section{Chunking module}
The chunking module provides an interface to perform document chunking on the ParsingResult tree representation of the \gls{pdf} document. We propose a novel solution aimed at increasing the traceability of the chunk content to its constituent ParsingResults. Prominent implementations of document chunking methods, such as the ones found in the \gls{rag} frameworks LlamaIndex \autocite{llamaindex} and Langchain \autocite{langchain}, treat the chunking process as a transformation of a single string, typically the Markdown representation of the document, into multiple smaller strings. Following this approach, resulting chunks lose their direct connection to the underlying structural elements, complicating source attribution.

Domain-specific implementations, such as Docling's Hybrid and Hierarchical Chunkers \autocite{docling,docling_toolkit}, improve upon this by including a list of elements associated with the resulting chunk. Additionally, they take advantage of the document's hierarchy by including relevant section headers in the chunk text. However, these implementations do not distinguish between partially and fully included elements. This results in chunk bounding boxes which always contain the entire element, regardless of how much of it's text is actually included in the text of the chunk. Furthermore, while the content of the section headers is included in the chunk, their corresponding bounding boxes are not retained.

Our solution addresses these limitations by enabling traceability at the highest relevant granularity for \gls{rag} chunking. While in traditional text segmentation, characters are the smallest instance that a text can be broken up at, the same can not be said about \gls{ai} models.\ \gls{ai} models are not directly constrained by the character length of their input and are instead limited by the amount of tokens that can fit into their context window. This means that the smallest instance that a text should be broken up into in the context of RAG is a token. To enable the tracing of every token inside the chunk to a token inside an element, we introduce the RichToken. A RichToken (\autoref{fig:rich-token}) is an object that wraps around a single token and it's original text, the id of the element that the token belongs to, and it's corresponding index inside that element. When the module performs the chunking operation on a ParsingResult, the specific chunking implementation processes the tree and returns the content of each chunk as a list of RichTokens. The module then creates a Chunk object using the RichTokens. By aggregating the content of the RichToken, the module can reconstruct the content of the entire Chunk, while maintaining the relationships to the constructing ParsingResults. Since the module is aware of the specific tokens that are included in the Chunk, we can determine tighter chunk bounding boxes using the span-level bounding boxes added to the elements in the \gls{dp} post-processing.

\begin{figure}[htpb]
  \centering
  \input{figures/code/rich_token.tex}
  \caption[RichToken]{Python implementation of the RichToken datatype.}\label{fig:rich-token}
\end{figure}

How the RichTokens are grouped together into chunks is to be decided by the specific chunking strategy. The module provides a \lstinline!_tokenize! function, which transforms a given ParsingResult node into RichTokens using the \lstinline!all-MiniLM-L6-v2! sentence transformer as a tokenizer. To prevent distinct document elements from merging into a single text block during chunk creation, \lstinline!\n! delimiters are appended to the content of the ParsingResult nodes before the chunking process begins. These delimiters act as textual representations of the breakpoints between document elements. Additionally, to avoid creating chunks, which are too big for the embedding module's context window, a limit $N$ for the maximum amount of tokens per chunk can be set on the chunking module. We provide implementations for four different document chunking strategies.

\subsection{Fixed-Size Chunking}
Fixed-size chunking segments the document into chunks of the chunking module's maximum chunk length $N$ with an overlap of $O$ tokens. It provides a simple and computationally efficient way to perform document chunking, while producing chunks of a constant size \autocite{semantic_chunking}. However, fixed-size chunking has no regard for the content of the document, which may result in chunk borders inside a single word or sentence \autocite{chunking_comparison}.

The strategy traverses the ParsingResult tree in reading order (e.g., depth-first), creating a queue of the document's RichTokens in the progress. When the queue reaches a length larger then $N$, the first $N$ tokens inside the queue are combined into a chunk while $N-O$ tokens are removed from the queue. This sliding window approach, leaving $O$ tokens inside the queue after the chunk is created, aims to maintain some contextual continuity between the chunks, leading to improved recall during the retrieval phase \autocite{semantic_chunking,chunking_comparison}.

\subsection{Recursive Character Chunking}
Recursive character chunking encapsulates a similar approach to fixed-size chunking. However, instead of naively setting the border of a chunk at a fixed token count, recursive character chunking utilizes an hierarchical list of delimiters (e.g., paragraphs, sentences, words) to define chunk boundaries \autocite{chunking_comparison,langchain_splitting_recursively,langchain}. When the RichToken queue exceeds the maximum chunk length $N$, the strategy splits the tokens using the highest priority delimiter. If there still exists a split which is larger than $N$, the process recurses on the oversized split with the next delimiter in the list. This `coarse-to-fine' approach preserves logical groupings while avoiding unnecessary fragmentation \autocite{chunking_comparison}. Similar to fixed-size chunking, recursive character chunking also incorporates sliding window chunking with an overlap of $O$ tokens between adjacent chunks. The delimiters used in this implementation are adapted from LangChain's \lstinline!RecursiveCharacterTextSplitter! \autocite{langchain,langchain_splitting_recursively}, with punctuation added to better identify sentence endings, as suggested by \textcite{chroma_eval}. This results in the following delimiters: \lstinline|[\"\n\n\", \"\n\", \".\", \"!\", \"?\", \" \", \"\"]|.

\subsection{Breakpoint-based Semantic Chunking}
Breakpoint-based semantic chunking separates the document at the sentence level, inserting breakpoints in between sentences to denote chunk borders \autocite{semantic_chunking}. Instead of relying on

For this strategy, the semantic distances between the embeddings of every adjacent sentence pair are calculated, with a high distance indicating a topical shift between the sentences. If the distance is larger than the $Q$-th percentile of all distances, a breakpoint is inserted.

\subsection{Hierarchical Chunking}

\section{Evaluation Framework}

\subsection{Document Layout Analysis evaluation}
The goal of the \gls{dla} evaluation is to assess the correctness of the bounding boxes and type labels produced by the parsing module \autocite{icdar2009}. While there are multiple datasets available for this task \autocite{docbank,doclaynet,omnidocbench}, we will use the PubLayNet dataset \autocite{publaynet} for our evaluation. While many datasets focus on evaluating \gls{dla} on a range of different document types such as forms, invoices or handwritten documents, PubLayNet consists solely of medical scientific articles \autocite{omnidocbench,publaynet}. This format closely resembles the format of the oncology guideline documents which makes it a suitable choice for this evaluation. Compromised of over 360.000 automatically annotated document pages collected from PubMed Central Open Access (PMCOA), PubLayNet is one of the largest datasets for \gls{dla}. As the dataset in its entirety is no longer publicly available and far too large for the purposes of this thesis, we will use \lstinline!publaynet-mini!, a small subset of 500 pages of the original dataset for this evaluation \autocite{publaynet-mini}. As seen in \autoref{tab:publaynet_counts}, the subset contains around 5000 ground truth annotations for elements from 5 different classes.

\begin{table}[htpb]
  \centering
  \input{figures/tables/publaynet_counts}
  \caption{Distribution of ground truth annotations across the different element types contained in the \lstinline!publaynet-mini! subset of the PubLayNet dataset.}\label{tab:publaynet_counts}
\end{table}

To assess the performance of different predictors on object detection tasks such as \gls{dla}, \gls{ap} is the most commonly used metric \autocite{object_detection_survey}. Previous evaluations of \gls{dla} models on the PubLayNet dataset also use a version of this metric \autocite{icdar2021_competition}.\ However, \gls{ap} relies on the predictor's confidence values, indicating how confident the predictor is about a predicted bounding box and class label. As most of the \gls{dp} implementations provide `hard-predictions', which do not contain any confidence values, the \gls{ap} is not a viable metric for the purposes of this thesis \autocite{lrp_error,eclair}.

For this reason, we will compare the implementations based on their achieved F1 score. Similarly to \gls{ap}, this metric takes into account two important measures for object detectors: precision and recall \autocite{object_detection_metrics}. According to \textcite{object_detection_metrics}, \textquote[p. 9]{Precision is the ability of a model to identify only relevant objects.\ \textelp{} Recall is the ability of a model to find all relevant cases \textelp{}}. In order to calculate their values, firstly the \glspl{dtbb} are classified into \glspl{tp} and \glspl{fp}. A \gls{dtbb} is classified as a \gls{tp} if there exists a \gls{gtbb} from the same class, so that their \gls{iou} is greater than a given threshold. One \gls{gtbb} can not be matched to multiple \glspl{dtbb}. If there does not exist a \gls{gtbb} that fulfils these criterions, the \gls{dtbb} is classified as a \gls{fp}. Any \glspl{gtbb} which were not matched to a \gls{dtbb} are classified as \glspl{fn}. Following the definition from \textcite{object_detection_metrics} for a model that, on a dataset with $G$ \glspl{gtbb}, outputs $N$ \glspl{dtbb}, out of which $S, (S\leq N)$ are \glspl{tp}, precision and recall can be formulated as shown in \autoref{eq:precision} and \autoref{eq:recall}.

\begin{equation}
  \text{Pr} = \frac{\sum_{n=1}^{S}\text{TP}_{n}}{\sum_{n=1}^{S}\text{TP}_{n} + \sum_{n=1}^{N-S}\text{FP}_{n}} = \frac{\sum_{n=1}^{S}\text{TP}_{n}}{all\ detections}
  \label{eq:precision}
\end{equation}

\begin{equation}
  \text{Re} = \frac{\sum_{n=1}^{S}\text{TP}_{n}}{\sum_{n=1}^{S}\text{TP}_{n} + \sum_{n=1}^{G-S}\text{FN}_{n}} = \frac{\sum_{n=1}^{S}\text{TP}_{n}}{all\ ground\ truths}
  \label{eq:recall}
\end{equation}

The F1 score is the weighted harmonic mean between precision and recall and is calculated as defined in \autoref{eq:f1score} \autocite{object_detection_metrics}. The F1 score is calculated for a single class at a set \gls{iou} threshold. Selecting a higher threshold will lead to a stricter metric as predictions need to be more precise to be counted as a \gls{tp} \autocite{object_detection_metrics}. A F1 score calculated at an \gls{iou} threshold T\% is commonly referred to as F1@T \autocite{scenescript}.

\begin{equation}
  F_1 = 2\frac{\text{Pr}\cdot\text{Rc}}{\text{Pr}+\text{Rc}}
  \label{eq:f1score}
\end{equation}

For scenarios with multiple classes, such as the PubLayNet dataset, the Macro F1 score can be used to assess the overall performance of the predictor \autocite{maximize_f1}. The Macro F1 score is the mean of the single class F1 scores. For a dataset with $M$ different classes, the calculation of the Macro F1 score is described in \autoref{eq:macrof1}. Hereby, $N_{:j}$ and $G_{:j}$ denote the \glspl{dtbb} and \glspl{gtbb} belonging to elements of class $j$ \autocite{maximize_f1}.

\begin{equation}
  F_{1_\text{Macro}}(N, G) = \frac{1}{M}\sum_{j=1}^{M}F_1(N_{:j}, G_{:j})
  \label{eq:macrof1}
\end{equation}

The \gls{dp} implementations will be evaluated on both their single-class and Macro F1 scores. Specifically, their (Macro) F1@50 and $F1@50:95$ will be compared against each other. F1@50:95 refers to the mean of the F1 values calculated at 10 evenly spaced \gls{iou} thresholds between 0.5 and 1.0 and is inspired by the primary challenge metric found in the MS COCO dataset \autocite{coco}. This rewards implementations, which provide more accurate bounding boxes \autocite{coco}. F1@50 is chosen, as a threshold of 50\% is one of the most commonly used threshold values for metrics in object detection \autocite{object_detection_metrics}. To calculate these metrics, the \lstinline!faster-coco-eval! package is used to determine the recall and precision values at the \gls{iou} thresholds \autocite{faster-coco-eval}.

\subsection{Content Parsing Evaluation}
OmniDocBench
Content:
\begin{itemize}
  \item Data Creation
  \item Evaluation Modes (End2End $to$ Evaluate Markdown output (Content))
  \item Usages in other papers
  \item State of the art evaluations (if I find any, most for all kinds of documents)
  \item Types of Documents (English and Chinese, different kinds: Scientific Paper\dots) all single page
  \item Subset for the thesis: English Scientific Papers
\end{itemize}

\subsection{Chunking Evaluation}
Chroma Evaluation